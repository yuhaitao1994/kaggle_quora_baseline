{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9fc5b9f39290b02e5b96b4ab84f82d983a6b7661"
   },
   "source": [
    "# kaggle_quora: baseline of yuhaitao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9733883580f2712c5d363a3d2cc2a60cdd83722c"
   },
   "source": [
    "比赛baseline\n",
    "\n",
    "参考:\n",
    "\n",
    "https://www.kaggle.com/shujian/single-rnn-with-4-folds-clr\n",
    "\n",
    "https://www.kaggle.com/gmhost/gru-capsule\n",
    "\n",
    "https://github.com/dennybritz/cnn-text-classification-tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample_submission.csv', 'test.csv', 'train.csv', 'embeddings', 'embeddings.zip']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "source": [
    "# load package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "5a7e5d3a5b563392d4767b0571e49fced06d38e0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from collections import Counter\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "35225af032b3e102500f73f5a0ac627259d67efa"
   },
   "source": [
    "# global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "d4943b2b4f45815bb0331a48bf27790737f22777"
   },
   "outputs": [],
   "source": [
    "data_dir = \"../input/\"\n",
    "train_file = os.path.join(data_dir, \"train.csv\")\n",
    "test_file = os.path.join(data_dir, \"test.csv\")\n",
    "embedding_size = 300\n",
    "max_len = 50\n",
    "max_features = 120000\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e2e7a749d9334c2d43f1b048f998f3e746a91618"
   },
   "source": [
    "# Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "8e7f108317a96c06e372e3c5564c0cb5ecbfb786"
   },
   "outputs": [],
   "source": [
    "# 将特殊字符单独挑出\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        if punct in x:\n",
    "#             x = x.replace(punct, f' {punct} ') # 这是python3.6语法\n",
    "            x = x.replace(punct, ' '+punct+' ')\n",
    "    return x\n",
    "\n",
    "# 清洗数字\n",
    "def clean_numbers(x):\n",
    "    if bool(re.search(r'\\d', x)):\n",
    "        x = re.sub('[0-9]{5,}', '#####', x)\n",
    "        x = re.sub('[0-9]{4}', '####', x)\n",
    "        x = re.sub('[0-9]{3}', '###', x)\n",
    "        x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "# 清洗拼写\n",
    "mispell_dict = {\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"I have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\" will\",\n",
    "\"didn't\": \"did not\",\n",
    "\"tryin'\":\"trying\"}\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "    return mispellings_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "503d0557f1c59d56dd0005fe960037d1d1ba06e3"
   },
   "outputs": [],
   "source": [
    "def load_and_prec():\n",
    "    train_df = pd.read_csv(train_file)\n",
    "    test_df = pd.read_csv(test_file)\n",
    "    print(\"Train shape : \",train_df.shape)\n",
    "    print(\"Test shape : \",test_df.shape)\n",
    "    \n",
    "    # 小写\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].str.lower()\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].str.lower()\n",
    "    \n",
    "    # 数字清洗\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: clean_numbers(x))\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: clean_numbers(x))\n",
    "    \n",
    "    # 清洗拼写\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: replace_typical_misspell(x))\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: replace_typical_misspell(x))\n",
    "    \n",
    "    # 数据清洗\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: clean_text(x))\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: clean_text(x))\n",
    "    \n",
    "    ## fill up the missing values\n",
    "    train_X = train_df[\"question_text\"].fillna(\"_##_\").values\n",
    "    test_X = test_df[\"question_text\"].fillna(\"_##_\").values\n",
    "\n",
    "    ## Tokenize the sentences\n",
    "    # 这个方法把所有字母都小写了\n",
    "    tokenizer = Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list(train_X))\n",
    "    train_X = tokenizer.texts_to_sequences(train_X)\n",
    "    test_X = tokenizer.texts_to_sequences(test_X)\n",
    "    \n",
    "    ## Get the target values\n",
    "    train_Y = train_df['target'].values\n",
    "    print(np.sum(train_Y))\n",
    "    \n",
    "#     # 在pad之前把前30个词去掉\n",
    "#     train_cut = []\n",
    "#     test_cut = []\n",
    "#     for x in train_X:\n",
    "#         train_cut.append([i for i in x if i>30])\n",
    "#     for x in test_X:\n",
    "#         test_cut.append([i for i in x if i>30])\n",
    "#     train_X = train_cut\n",
    "#     test_X = test_cut\n",
    "\n",
    "    ## Pad the sentences \n",
    "    train_X = pad_sequences(train_X, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "    test_X = pad_sequences(test_X, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "    \n",
    "# #     # 把最常用的40个词去掉，pad为0\n",
    "# #     train_X = np.where(train_X>=40, train_X, 0)\n",
    "# #     test_X = np.where(test_X>=40, test_X, 0)\n",
    "    \n",
    "    #shuffling the data\n",
    "    np.random.seed(20190101)\n",
    "    trn_idx = np.random.permutation(len(train_X))\n",
    "\n",
    "    train_X = train_X[trn_idx]\n",
    "    train_Y = train_Y[trn_idx]\n",
    "    \n",
    "    print(train_X.shape)\n",
    "    print(train_Y.shape)\n",
    "    print(test_X.shape)\n",
    "    print(len(tokenizer.word_index))\n",
    "    \n",
    "    return train_X, test_X, train_Y, tokenizer.word_index\n",
    "\n",
    "# load_and_prec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "22473b291e72dffdd767ebf22f2b7d3c1ab3bacf"
   },
   "source": [
    "# load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "832e035728a8c49019b0ae6f1a43ae1f3d141a8e"
   },
   "outputs": [],
   "source": [
    "def load_glove(word_index):\n",
    "    EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix\n",
    "\n",
    "def load_fasttext(word_index):\n",
    "    \"\"\"\n",
    "    这个加载词向量还没有细看\n",
    "    \"\"\"\n",
    "    EMBEDDING_FILE = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "    \n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "def load_para(word_index):\n",
    "    EMBEDDING_FILE = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100 and o.split(\" \")[0] in word_index)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "    \n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2f4a9363d9c95957d88f7ce809f662d3729c1b08"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f462265e51ac8a3a61e6a5560c803faf3788e12e"
   },
   "source": [
    "text_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "bcbaf2e1ca63978750a9fdce1a0a8526139d4f08"
   },
   "outputs": [],
   "source": [
    "class model_text_cnn(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_matrix, sequence_length=60, num_classes=1,\n",
    "                 embedding_size=300, filter_sizes=[1,3,5], num_filters=256, trainable=True):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.int32, [None], name=\"input_y\")\n",
    "        self.keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "        \n",
    "        # Some variables\n",
    "        self.embedding_matrix = tf.get_variable(\"embedding_matrix\", initializer=tf.constant(\n",
    "                embedding_matrix, dtype=tf.float32), trainable=False)\n",
    "        self.global_step = tf.get_variable('global_step', shape=[], dtype=tf.int32,\n",
    "                                           initializer=tf.constant_initializer(0), trainable=False)\n",
    "        \n",
    "        # The structure of the model\n",
    "        self.layers(num_classes, filter_sizes, num_filters)\n",
    "        # optimizer\n",
    "        if trainable:\n",
    "#             self.learning_rate = tf.get_variable(\n",
    "#                     \"learning_rate\", shape=[], dtype=tf.float32, trainable=False)\n",
    "            self.learning_rate = tf.train.exponential_decay(\n",
    "                learning_rate=0.001, global_step=self.global_step, decay_steps=1000, decay_rate=0.95)\n",
    "            self.opt = tf.train.AdamOptimizer(learning_rate=self.learning_rate, epsilon=1e-8)\n",
    "            self.train_op = self.opt.minimize(self.loss, global_step=self.global_step)\n",
    "    \n",
    "    def layers(self, num_classes, filter_sizes, num_filters):\n",
    "        # Embedding layer\n",
    "        with tf.variable_scope(\"embedding\"):\n",
    "            self.embedding_inputs = tf.nn.embedding_lookup(self.embedding_matrix, self.input_x)\n",
    "            self.embedding_inputs = tf.nn.dropout(self.embedding_inputs, self.keep_prob)\n",
    "        \n",
    "        with tf.variable_scope(\"convolutional_layers\"):\n",
    "            \"\"\"\n",
    "            卷积和池化操作\n",
    "            \"\"\"\n",
    "            self.pooled_outputs = []\n",
    "            for i, filter_size in enumerate(filter_sizes):\n",
    "                with tf.variable_scope(\"conv-maxpool-{}\".format(filter_size)):\n",
    "                    # conv\n",
    "                    filter_W = tf.get_variable(shape=[filter_size, embedding_size, num_filters], name=\"filter_W\", dtype=tf.float32)\n",
    "                    filter_b = tf.get_variable(shape=[num_filters], name=\"filter_b\", dtype=tf.float32)\n",
    "                    conv = tf.nn.conv1d(\n",
    "                        value=self.embedding_inputs,\n",
    "                        filters=filter_W,\n",
    "                        stride=1,\n",
    "                        padding=\"VALID\",\n",
    "                        name=\"conv\") # shape:[batch_size, seq_len-filter_size+1, num_filters]\n",
    "                    # relu\n",
    "                    h = tf.nn.relu(tf.nn.bias_add(conv, filter_b), name=\"relu\")\n",
    "                    # global max pooling layer\n",
    "                    pooled = tf.reduce_max(h, axis=1, name='global_max_pooling') # shape: [batch_size, num_filters]\n",
    "                    self.pooled_outputs.append(pooled)\n",
    "        \n",
    "            # Combine all the pooled features\n",
    "            self.conv_outputs = tf.concat(self.pooled_outputs, axis=1)\n",
    "            # dropout\n",
    "            self.conv_outputs_drop = tf.nn.dropout(self.conv_outputs, self.keep_prob)\n",
    "            \n",
    "#         with tf.variable_scope(\"fully_connected\"):\n",
    "#             \"\"\"\n",
    "#             全连接层\n",
    "#             \"\"\"\n",
    "#             fc_W = tf.get_variable(\n",
    "#                     shape=[self.conv_outputs_drop.get_shape().as_list()[1], num_classes],\n",
    "#                     initializer=tf.contrib.layers.xavier_initializer(),\n",
    "#                     name=\"fc_w\")\n",
    "#             fc_b = tf.get_variable(shape=[num_classes], initializer=tf.constant_initializer(0.1), name=\"fc_b\")\n",
    "#             self.logits = tf.nn.bias_add(tf.matmul(self.conv_outputs_drop, fc_W), fc_b)\n",
    "            \n",
    "#         with tf.variable_scope(\"softmax_and_loss\"):\n",
    "#             \"\"\"\n",
    "#             softmax输出层并计算loss\n",
    "#             \"\"\"\n",
    "#             self.softmax = tf.nn.softmax(self.logits)\n",
    "#             self.classes = tf.cast(tf.argmax(self.softmax, axis=1), dtype=tf.int32, name=\"classes\")\n",
    "#             self.loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "#                     logits=self.logits, labels=self.input_y))\n",
    "        \n",
    "        with tf.variable_scope(\"fully_connected\"):\n",
    "            \"\"\"\n",
    "            全连接层\n",
    "            \"\"\"\n",
    "            fc_W1 = tf.get_variable(\n",
    "                    shape=[self.conv_outputs_drop.get_shape().as_list()[1], 128],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                    name=\"fc_w1\")\n",
    "            fc_b1 = tf.get_variable(shape=[128], initializer=tf.constant_initializer(0.1), name=\"fc_b1\")\n",
    "            fc_1 = tf.nn.relu(tf.nn.bias_add(tf.matmul(self.conv_outputs_drop, fc_W1), fc_b1))\n",
    "            fc_1_drop = tf.nn.dropout(fc_1, self.keep_prob)\n",
    "            \n",
    "            fc_W2 = tf.get_variable(\n",
    "                    shape=[128, num_classes],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                    name=\"fc_w2\")\n",
    "            fc_b2 = tf.get_variable(shape=[num_classes], initializer=tf.constant_initializer(0.1), name=\"fc_b2\")\n",
    "            self.logits = tf.squeeze(tf.nn.bias_add(tf.matmul(fc_1_drop, fc_W2), fc_b2), name=\"logits\")\n",
    "        \n",
    "        with tf.variable_scope(\"sigmoid_and_loss\"):\n",
    "            \"\"\"\n",
    "            用sigmoid函数加阈值代替softmax的多分类\n",
    "            \"\"\"\n",
    "            self.sigmoid = tf.nn.sigmoid(self.logits)\n",
    "            self.classes = tf.cast(tf.greater(self.sigmoid, 0.34), dtype=tf.int32, name=\"classes\")\n",
    "            self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    logits=self.logits, labels=tf.cast(self.input_y, dtype=tf.float32)))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "acc7a63f9f42fc9663fdda5a95deec3cf4529f46"
   },
   "source": [
    "# Training Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "e7378b5d2e75f287761852b124a24e96d7febc0e"
   },
   "outputs": [],
   "source": [
    "# batch生成器\n",
    "def batch_generator(train_X, train_Y, batch_size, is_train=True):\n",
    "    \"\"\"\n",
    "    batch生成器:\n",
    "    在is_train为true的情况下，补充batch，并shuffle\n",
    "    \"\"\"\n",
    "    data_number = train_X.shape[0]\n",
    "    batch_count = 0\n",
    "    while True:\n",
    "        if batch_count * batch_size + batch_size > data_number:\n",
    "            # 最后一个batch的操作\n",
    "            if is_train:\n",
    "                # 后面的直接舍弃，重新开始\n",
    "                # shuffle\n",
    "                np.random.seed(2018)\n",
    "                trn_idx = np.random.permutation(data_number)\n",
    "                train_X = train_X[trn_idx]\n",
    "                train_Y = train_Y[trn_idx]\n",
    "                one_batch_X = train_X[0:batch_size]\n",
    "                one_batch_Y = train_Y[0:batch_size]\n",
    "                batch_count = 1\n",
    "                yield one_batch_X, one_batch_Y\n",
    "            else:\n",
    "                one_batch_X = train_X[batch_count * batch_size:data_number]\n",
    "                one_batch_Y = train_Y[batch_count * batch_size:data_number]\n",
    "                batch_count = 0\n",
    "                yield one_batch_X, one_batch_Y\n",
    "        else:\n",
    "            one_batch_X = train_X[batch_count * batch_size:batch_count * batch_size + batch_size]\n",
    "            one_batch_Y = train_Y[batch_count * batch_size:batch_count * batch_size + batch_size]\n",
    "            batch_count += 1\n",
    "            yield one_batch_X, one_batch_Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "9d659841002ebc1ee59c9de724257521b5f98501"
   },
   "outputs": [],
   "source": [
    "def f1_smart(y_true, y_pred):\n",
    "    args = np.argsort(y_pred)\n",
    "    tp = y_true.sum()\n",
    "    fs = (tp - np.cumsum(y_true[args[:-1]])) / np.arange(y_true.shape[0] + tp - 1, tp, -1)\n",
    "    res_idx = np.argmax(fs)\n",
    "    return 2 * fs[res_idx], (y_pred[args[res_idx]] + y_pred[args[res_idx + 1]]) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "5e9bdd89effe902f5410ce4089cd65fea5d214d0"
   },
   "outputs": [],
   "source": [
    "# 正类欠采样，负类数据增强，暂时用随机打乱数据增强.\n",
    "def data_augmentation(X, Y, under_sample=100000, aug_num=3):\n",
    "    \"\"\"\n",
    "    under_sample: 欠采样个数\n",
    "    aug: 数据增强倍数\n",
    "    \"\"\"\n",
    "    pos_X = []\n",
    "    neg_X = []\n",
    "    for i in range(X.shape[0]):\n",
    "        if Y[i] == 1:\n",
    "            neg_X.append(list(X[i]))\n",
    "        else:\n",
    "            pos_X.append(list(X[i]))\n",
    "    \n",
    "    # 正样本欠采样\n",
    "    random.shuffle(pos_X)\n",
    "    pos_X = pos_X[:-under_sample]\n",
    "    \n",
    "    # 正样本数据增强\n",
    "    pos_X_aug = []\n",
    "    for i in range(200000):\n",
    "        aug = []\n",
    "        for x in pos_X[i]:\n",
    "            if x != 0:\n",
    "                aug.append(x)\n",
    "            else:\n",
    "                break\n",
    "        random.shuffle(aug)\n",
    "        aug += [0] * (max_len-len(aug))\n",
    "        pos_X_aug.append(aug)\n",
    "    pos_X.extend(pos_X_aug)\n",
    "    print(len(pos_X))\n",
    "    \n",
    "    # 负样本数据增强\n",
    "    neg_X_aug = []\n",
    "    for i in range(aug_num):\n",
    "        for neg in neg_X:\n",
    "            aug = []\n",
    "            for x in neg:\n",
    "                if x != 0:\n",
    "                    aug.append(x)\n",
    "                else:\n",
    "                    break\n",
    "            random.shuffle(aug)\n",
    "            aug += [0] * (max_len-len(aug))\n",
    "            neg_X_aug.append(aug)\n",
    "        \n",
    "    neg_X.extend(neg_X_aug)\n",
    "    print(len(neg_X))\n",
    "    \n",
    "    pos_Y = np.zeros(shape=[len(pos_X)], dtype=np.int32)\n",
    "    neg_Y = np.ones(shape=[len(neg_X)], dtype=np.int32)\n",
    "    \n",
    "    pos_X.extend(neg_X)\n",
    "    X_out = np.array(pos_X, dtype=np.int32)\n",
    "    Y_out = np.append(pos_Y, neg_Y)\n",
    "    \n",
    "    print(X_out.shape)\n",
    "    #shuffling the data\n",
    "    np.random.seed(2018)\n",
    "    trn_idx = np.random.permutation(len(X_out))\n",
    "\n",
    "    X_out = X_out[trn_idx]\n",
    "    Y_out = Y_out[trn_idx]\n",
    "    \n",
    "    print(X_out.shape)\n",
    "    print(Y_out.shape)\n",
    "\n",
    "    return X_out, Y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5aa54cfdc63067b4f45036f0b2030b676175ac2b"
   },
   "source": [
    "# Main part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "7c071e1fe53195a1e9057e1b98446f7e60fe7ea1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1306122, 3)\n",
      "Test shape :  (56370, 2)\n",
      "80810\n",
      "(1306122, 50)\n",
      "(1306122,)\n",
      "(56370, 50)\n",
      "185946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(120000, 300)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载数据，平均词向量\n",
    "train_X, test_X, train_Y, word_index = load_and_prec()\n",
    "# embedding_matrix_1 = load_glove(word_index)\n",
    "embedding_matrix = load_fasttext(word_index)\n",
    "# embedding_matrix_3 = load_para(word_index)\n",
    "# embedding_matrix = np.mean([embedding_matrix_1, embedding_matrix_2], axis = 0)\n",
    "np.shape(embedding_matrix)\n",
    "# embedding_matrix = np.zeros(shape=[100,300],dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "2ddbe850cbf1ec19beb3c4027a489f840725b22a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps:1000, train_loss:0.13604, val_loss:0.12437, val_F1:0.60239, val_pre:0.48056, val_recall:0.80695\n",
      "test done!\n",
      "steps:2000, train_loss:0.11866, val_loss:0.10965, val_F1:0.63923, val_pre:0.55719, val_recall:0.74960\n",
      "test done!\n",
      "steps:3000, train_loss:0.11334, val_loss:0.10757, val_F1:0.64843, val_pre:0.56125, val_recall:0.76766\n",
      "test done!\n",
      "steps:4000, train_loss:0.11117, val_loss:0.10566, val_F1:0.65297, val_pre:0.57283, val_recall:0.75919\n",
      "test done!\n",
      "steps:5000, train_loss:0.10759, val_loss:0.10408, val_F1:0.66258, val_pre:0.59003, val_recall:0.75548\n",
      "test done!\n",
      "steps:6000, train_loss:0.10741, val_loss:0.10472, val_F1:0.66013, val_pre:0.57255, val_recall:0.77936\n",
      "steps:7000, train_loss:0.10366, val_loss:0.10318, val_F1:0.66507, val_pre:0.59479, val_recall:0.75418\n",
      "test done!\n",
      "steps:8000, train_loss:0.10365, val_loss:0.10343, val_F1:0.66119, val_pre:0.56751, val_recall:0.79192\n",
      "steps:9000, train_loss:0.10008, val_loss:0.10170, val_F1:0.66969, val_pre:0.59114, val_recall:0.77231\n",
      "test done!\n",
      "steps:10000, train_loss:0.10076, val_loss:0.10083, val_F1:0.67389, val_pre:0.61137, val_recall:0.75065\n",
      "test done!\n",
      "steps:11000, train_loss:0.09743, val_loss:0.10225, val_F1:0.66879, val_pre:0.58256, val_recall:0.78499\n",
      "steps:12000, train_loss:0.09733, val_loss:0.10175, val_F1:0.66927, val_pre:0.58668, val_recall:0.77893\n",
      "steps:13000, train_loss:0.09518, val_loss:0.10088, val_F1:0.67141, val_pre:0.59115, val_recall:0.77688\n",
      "steps:14000, train_loss:0.09529, val_loss:0.10031, val_F1:0.67477, val_pre:0.60981, val_recall:0.75523\n",
      "test done!\n",
      "steps:15000, train_loss:0.09307, val_loss:0.10376, val_F1:0.66150, val_pre:0.55841, val_recall:0.81129\n",
      "steps:16000, train_loss:0.09249, val_loss:0.10127, val_F1:0.66929, val_pre:0.58149, val_recall:0.78833\n",
      "steps:17000, train_loss:0.08962, val_loss:0.10042, val_F1:0.67436, val_pre:0.60626, val_recall:0.75968\n",
      "steps:18000, train_loss:0.09134, val_loss:0.10232, val_F1:0.66419, val_pre:0.56719, val_recall:0.80120\n",
      "steps:19000, train_loss:0.08906, val_loss:0.10121, val_F1:0.67367, val_pre:0.60990, val_recall:0.75232\n",
      "steps:20000, train_loss:0.08841, val_loss:0.10242, val_F1:0.66722, val_pre:0.57226, val_recall:0.79996\n"
     ]
    }
   ],
   "source": [
    "# 多折训练，交叉验证平均，测试\n",
    "\n",
    "# 划分交叉验证集\n",
    "DATA_SPLIT_SEED = 20190101\n",
    "splits = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=DATA_SPLIT_SEED).split(train_X, train_Y))\n",
    "\n",
    "# 最终输出\n",
    "best_test = []\n",
    "\n",
    "# 多折训练\n",
    "for i, (train_idx, valid_idx) in enumerate(splits):\n",
    "    X_train = train_X[train_idx]\n",
    "    Y_train = train_Y[train_idx]\n",
    "    X_val = train_X[valid_idx]\n",
    "    Y_val = train_Y[valid_idx]\n",
    "    \n",
    "#     # 数据增强\n",
    "#     X_train, Y_train = data_augmentation(X_train, Y_train) \n",
    "#     print(Y_train[:100])\n",
    "#     print(Y_train[-100:])\n",
    "    \n",
    "    # batch生成器\n",
    "    train_batch = batch_generator(X_train, Y_train, batch_size, True)\n",
    "    val_batch = batch_generator(X_val, Y_val, batch_size, False)\n",
    "    test_batch = batch_generator(test_X, np.zeros(shape=[test_X.shape[0]], dtype=np.int32), batch_size, False)\n",
    "    \n",
    "    # 选择最好的结果\n",
    "    best_val_f1 = 0.0\n",
    "    \n",
    "    # 训练 & 验证 & 测试\n",
    "    with tf.Graph().as_default():\n",
    "        sess_config = tf.ConfigProto(allow_soft_placement=True)\n",
    "        sess_config.gpu_options.allow_growth = True\n",
    "        with tf.Session(config=sess_config) as sess:\n",
    "            writer = tf.summary.FileWriter(\"./log/\", sess.graph)\n",
    "            # 模型\n",
    "            model = model_text_cnn(embedding_matrix=embedding_matrix, sequence_length=max_len)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "#             sess.run(tf.assign(model.learning_rate, tf.constant(0.001, dtype=tf.float32)))\n",
    "            \n",
    "            train_loss_sum = 0.0\n",
    "            for go in range(20000):\n",
    "                steps = sess.run(model.global_step) + 1\n",
    "                # 训练\n",
    "                train_batch_X, train_batch_Y = next(train_batch)\n",
    "                feed = {model.input_x:train_batch_X, model.input_y:train_batch_Y, model.keep_prob:0.7}\n",
    "                loss, train_op = sess.run([model.loss, model.train_op], feed_dict=feed)\n",
    "                train_loss_sum += loss\n",
    "                # 验证 & 测试\n",
    "                if steps % 1000 == 0: \n",
    "                    val_predictions = []\n",
    "                    val_loss_sum = 0.0\n",
    "                    for _ in range(X_val.shape[0] // batch_size + 1):\n",
    "                        val_batch_X, val_batch_Y = next(val_batch)\n",
    "                        feed_val = {model.input_x:val_batch_X, model.input_y:val_batch_Y, model.keep_prob:1.0}\n",
    "                        val_loss, val_classes = sess.run([model.loss, model.classes], feed_dict=feed_val)\n",
    "                        val_predictions.extend(val_classes)\n",
    "                        val_loss_sum += val_loss\n",
    "                    val_f1 = metrics.f1_score(Y_val, np.array(val_predictions))\n",
    "#                     val_f1, _ = f1_smart(Y_val, np.array(val_predictions))\n",
    "                    val_pre = metrics.precision_score(Y_val, np.array(val_predictions))\n",
    "                    val_recall = metrics.recall_score(Y_val, np.array(val_predictions))\n",
    "                    val_loss_sum = val_loss_sum / (X_val.shape[0] // batch_size + 1)\n",
    "                    print(\"steps:{}, train_loss:{:.5f}, val_loss:{:.5f}, val_F1:{:.5f}, val_pre:{:.5f}, val_recall:{:.5f}\".format(\n",
    "                        steps, float(train_loss_sum / 1000), float(val_loss_sum), float(val_f1), float(val_pre), float(val_recall)))\n",
    "                    \n",
    "                    # 写入tensorboard\n",
    "                    train_loss_write = tf.Summary(value=[tf.Summary.Value(tag=\"model/train_loss\", \\\n",
    "                                                                          simple_value=train_loss_sum / 1000), ])\n",
    "                    writer.add_summary(train_loss_write, steps)\n",
    "                    val_loss_write = tf.Summary(value=[tf.Summary.Value(tag=\"model/val_loss\", simple_value=val_loss_sum), ])\n",
    "                    writer.add_summary(val_loss_write, steps)\n",
    "                    val_f1_write = tf.Summary(value=[tf.Summary.Value(tag=\"index/val_f1\", simple_value=val_f1), ])\n",
    "                    writer.add_summary(val_f1_write, steps)\n",
    "                    val_pre_write = tf.Summary(value=[tf.Summary.Value(tag=\"index/val_precision\", simple_value=val_pre), ])\n",
    "                    writer.add_summary(val_pre_write, steps)\n",
    "                    val_recall_write = tf.Summary(value=[tf.Summary.Value(tag=\"index/val_recall\", simple_value=val_recall), ])\n",
    "                    writer.add_summary(val_recall_write, steps)\n",
    "                    writer.flush()\n",
    "                    \n",
    "                    # train loss\n",
    "                    train_loss_sum = 0.0\n",
    "                    \n",
    "                    # 测试，并选取最好的F1值的时刻的测试结果为最终结果\n",
    "                    if val_f1 > best_val_f1:\n",
    "                        best_val_f1 = val_f1\n",
    "                        best_test = []\n",
    "                        for _ in range(test_X.shape[0] // batch_size + 1):\n",
    "                            test_batch_X, _ = next(test_batch)\n",
    "                            feed_test = {model.input_x:test_batch_X, model.keep_prob:1.0}\n",
    "                            test_classes = sess.run(model.classes, feed_dict=feed_test)\n",
    "                            best_test.extend(test_classes)\n",
    "                        print(\"test done!\")\n",
    "    break # 先只用一折测试\n",
    "\n",
    "sub = pd.read_csv('../input/sample_submission.csv')\n",
    "sub[\"prediction\"] = best_test\n",
    "sub.to_csv(\"submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6f00eee9e835f34a1a70ff828f688f56a8509556"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
