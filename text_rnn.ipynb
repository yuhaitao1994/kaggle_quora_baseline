{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kaggle_quora: single model of yuhaitao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比赛baseline\n",
    "\n",
    "参考:\n",
    "\n",
    "https://www.kaggle.com/shujian/single-rnn-with-4-folds-clr\n",
    "\n",
    "https://www.kaggle.com/gmhost/gru-capsule\n",
    "\n",
    "https://github.com/dennybritz/cnn-text-classification-tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample_submission.csv', 'test.csv', 'train.csv', 'embeddings', 'embeddings.zip']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "source": [
    "# load package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from collections import Counter\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../input/\"\n",
    "train_file = os.path.join(data_dir, \"train.csv\")\n",
    "test_file = os.path.join(data_dir, \"test.csv\")\n",
    "embedding_size = 300\n",
    "max_len = 50\n",
    "max_features = 120000\n",
    "batch_size = 512\n",
    "use_local_test = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将特殊字符单独挑出\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        if punct in x:\n",
    "#             x = x.replace(punct, f' {punct} ') # 这是python3.6语法\n",
    "            x = x.replace(punct, ' '+punct+' ')\n",
    "    return x\n",
    "\n",
    "# 清洗数字\n",
    "def clean_numbers(x):\n",
    "    if bool(re.search(r'\\d', x)):\n",
    "        x = re.sub('[0-9]{5,}', '#####', x)\n",
    "        x = re.sub('[0-9]{4}', '####', x)\n",
    "        x = re.sub('[0-9]{3}', '###', x)\n",
    "        x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "# 清洗拼写\n",
    "mispell_dict = {\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"I have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\" will\",\n",
    "\"didn't\": \"did not\",\n",
    "\"tryin'\":\"trying\"}\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "    return mispellings_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prec(use_local_test=True):\n",
    "    train_df = pd.read_csv(train_file)\n",
    "    test_df = pd.read_csv(test_file)\n",
    "    print(\"Train shape : \",train_df.shape)\n",
    "    print(\"Test shape : \",test_df.shape)\n",
    "    display(train_df.head())\n",
    "    display(test_df.head())\n",
    "    \n",
    "    # 小写\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].str.lower()\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].str.lower()\n",
    "    \n",
    "    # 数字清洗\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: clean_numbers(x))\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: clean_numbers(x))\n",
    "    \n",
    "    # 清洗拼写\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: replace_typical_misspell(x))\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: replace_typical_misspell(x))\n",
    "    \n",
    "    # 数据清洗\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: clean_text(x))\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: clean_text(x))\n",
    "    \n",
    "    ## fill up the missing values\n",
    "    train_X = train_df[\"question_text\"].fillna(\"_##_\").values\n",
    "    test_X = test_df[\"question_text\"].fillna(\"_##_\").values\n",
    "\n",
    "    ## Tokenize the sentences\n",
    "    # 这个方法把所有字母都小写了\n",
    "    tokenizer = Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list(train_X))\n",
    "    train_X = tokenizer.texts_to_sequences(train_X)\n",
    "    test_X = tokenizer.texts_to_sequences(test_X)\n",
    "    \n",
    "    ## Get the target values\n",
    "    train_Y = train_df['target'].values\n",
    "    print(np.sum(train_Y))\n",
    "    \n",
    "#     # 在pad之前把前30个词去掉\n",
    "#     train_cut = []\n",
    "#     test_cut = []\n",
    "#     for x in train_X:\n",
    "#         train_cut.append([i for i in x if i>30])\n",
    "#     for x in test_X:\n",
    "#         test_cut.append([i for i in x if i>30])\n",
    "#     train_X = train_cut\n",
    "#     test_X = test_cut\n",
    "\n",
    "    ## Pad the sentences \n",
    "    train_X = pad_sequences(train_X, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "    test_X = pad_sequences(test_X, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "    \n",
    "# #     # 把最常用的40个词去掉，pad为0\n",
    "# #     train_X = np.where(train_X>=40, train_X, 0)\n",
    "# #     test_X = np.where(test_X>=40, test_X, 0)\n",
    "    \n",
    "    #shuffling the data\n",
    "    np.random.seed(20190101)\n",
    "    trn_idx = np.random.permutation(len(train_X))\n",
    "\n",
    "    train_X = train_X[trn_idx]\n",
    "    train_Y = train_Y[trn_idx]\n",
    "    \n",
    "    # 使用本地测试集\n",
    "    if use_local_test:\n",
    "        train_X, local_test_X = (train_X[:-4*len(test_X)], train_X[-4*len(test_X):])\n",
    "        train_Y, local_test_Y = (train_Y[:-4*len(test_X)], train_Y[-4*len(test_X):])\n",
    "    else:\n",
    "        local_test_X = np.zeros(shape=[1,max_len], dtype=np.int32)\n",
    "        local_test_Y = np.zeros(shape=[1], dtype=np.int32)\n",
    "        \n",
    "    print(train_X.shape)\n",
    "    print(local_test_X.shape)\n",
    "    print(test_X.shape)\n",
    "    print(len(tokenizer.word_index))\n",
    "    \n",
    "    return train_X, test_X, train_Y, local_test_X, local_test_Y, tokenizer.word_index\n",
    "\n",
    "# load_and_prec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(word_index):\n",
    "    EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix\n",
    "\n",
    "def load_fasttext(word_index):\n",
    "    \"\"\"\n",
    "    这个加载词向量还没有细看\n",
    "    \"\"\"\n",
    "    EMBEDDING_FILE = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "    \n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "def load_para(word_index):\n",
    "    EMBEDDING_FILE = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100 and o.split(\" \")[0] in word_index)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "    \n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text_rnn(Bi-GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(args, keep_prob, is_train, mode=\"recurrent\"):\n",
    "    \"\"\"\n",
    "    dropout层,args初始是1.0\n",
    "    \"\"\"\n",
    "    noise_shape = None\n",
    "    scale = 1.0\n",
    "    shape = tf.shape(args)\n",
    "    if mode == \"recurrent\" and len(args.get_shape().as_list()) == 3:\n",
    "        noise_shape = [shape[0], 1, shape[-1]]\n",
    "    args = tf.cond(is_train, lambda: tf.nn.dropout(\n",
    "        args, keep_prob, noise_shape=noise_shape) * scale, lambda: args)\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cudnn_gru:\n",
    "    def __init__(self, num_layers, num_units, input_size, scope=None):\n",
    "        self.num_layers = num_layers\n",
    "        self.grus = []\n",
    "        self.inits = []\n",
    "        self.dropout_mask = []\n",
    "        self.scope = scope\n",
    "        for layer in range(num_layers):\n",
    "            input_size_ = input_size if layer == 0 else 2 * num_units\n",
    "            gru_fw = tf.contrib.cudnn_rnn.CudnnGRU(\n",
    "                1, num_units, name=\"f_cudnn_gru\")\n",
    "            gru_bw = tf.contrib.cudnn_rnn.CudnnGRU(\n",
    "                1, num_units, name=\"b_cudnn_gru\")\n",
    "            self.grus.append((gru_fw, gru_bw, ))\n",
    "\n",
    "    def __call__(self, inputs, seq_len, keep_prob, concat_layers=True):\n",
    "        # cudnn GRU需要交换张量的维度，可能是便于计算\n",
    "        outputs = [tf.transpose(inputs, [1, 0, 2])]\n",
    "        out_states = []\n",
    "        with tf.variable_scope(self.scope):\n",
    "            for layer in range(self.num_layers):\n",
    "                gru_fw, gru_bw = self.grus[layer]\n",
    "                with tf.variable_scope(\"fw_{}\".format(layer)):\n",
    "                    out_fw, (fw_state,) = gru_fw(outputs[-1])\n",
    "                with tf.variable_scope(\"bw_{}\".format(layer)):\n",
    "                    inputs_bw = tf.reverse_sequence(outputs[-1], seq_lengths=seq_len, seq_dim=0, batch_dim=1)\n",
    "                    out_bw, (bw_state,) = gru_bw(inputs_bw)\n",
    "                    out_bw = tf.reverse_sequence(out_bw, seq_lengths=seq_len, seq_dim=0, batch_dim=1)\n",
    "                outputs.append(tf.concat([out_fw, out_bw], axis=2))\n",
    "                out_states.append(tf.concat([fw_state, bw_state], axis=-1))\n",
    "        if concat_layers:\n",
    "            res = tf.concat(outputs[1:], axis=2)\n",
    "            final_state = tf.squeeze(tf.transpose(tf.concat(out_states, axis=0), [1,0,2]), axis=1)\n",
    "        else:\n",
    "            res = outputs[-1]\n",
    "            final_state = tf.squeeze(out_states[-1], axis=0)\n",
    "        res = tf.transpose(res, [1, 0, 2])\n",
    "        return res, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_text_rnn(object):\n",
    "    \"\"\"\n",
    "    使用简单的双向GRU实现分类。\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_matrix, sequence_length=50, num_classes=1,\n",
    "                 embedding_size=300, trainable=True):\n",
    "        \n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.int32, [None], name=\"input_y\")\n",
    "        self.keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "        \n",
    "        # Some variables\n",
    "        self.embedding_matrix = tf.get_variable(\"embedding_matrix\", initializer=tf.constant(\n",
    "                embedding_matrix, dtype=tf.float32), trainable=False)\n",
    "        self.global_step = tf.get_variable('global_step', shape=[], dtype=tf.int32,\n",
    "                                           initializer=tf.constant_initializer(0), trainable=False)\n",
    "        \n",
    "        with tf.name_scope(\"process\"):\n",
    "            self.seq_len = tf.reduce_sum(tf.cast(tf.cast(self.input_x, dtype=tf.bool), dtype=tf.int32), axis=1, name=\"seq_len\")\n",
    "        \n",
    "        # The structure of the model\n",
    "        self.layers(num_classes)\n",
    "        # optimizer\n",
    "        if trainable:\n",
    "            self.learning_rate = tf.train.exponential_decay(\n",
    "                learning_rate=0.001, global_step=self.global_step, decay_steps=1000, decay_rate=0.95)\n",
    "            self.opt = tf.train.AdamOptimizer(learning_rate=self.learning_rate, epsilon=1e-8)\n",
    "            self.train_op = self.opt.minimize(self.loss, global_step=self.global_step)\n",
    "        \n",
    "    def layers(self, num_classes):\n",
    "        # Embedding layer\n",
    "        with tf.variable_scope(\"embedding\"):\n",
    "            self.embedding_inputs = tf.nn.embedding_lookup(self.embedding_matrix, self.input_x)\n",
    "            self.embedding_inputs = tf.nn.dropout(self.embedding_inputs, self.keep_prob)\n",
    "        \n",
    "        # Bi-GRU Encoder\n",
    "        with tf.variable_scope(\"Bi-GRU\"):\n",
    "            bi_rnn = cudnn_gru(num_layers=1, num_units=128, input_size=self.embedding_inputs.get_shape().as_list()[-1], scope=\"encoder\")\n",
    "            _, final_state = bi_rnn(self.embedding_inputs, seq_len=self.seq_len, keep_prob=self.keep_prob) \n",
    "            # shape: [batch_size, 2*hidden]\n",
    "            self.rnn_out = tf.nn.dropout(final_state, keep_prob=self.keep_prob)\n",
    "            \n",
    "        with tf.variable_scope(\"fully_connected\"):\n",
    "            \"\"\"\n",
    "            全连接层\n",
    "            \"\"\"\n",
    "            fc_W1 = tf.get_variable(\n",
    "                    shape=[self.rnn_out.get_shape().as_list()[1], 128],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                    name=\"fc_w1\")\n",
    "            fc_b1 = tf.get_variable(shape=[128], initializer=tf.constant_initializer(0.1), name=\"fc_b1\")\n",
    "            fc_1 = tf.nn.relu(tf.nn.bias_add(tf.matmul(self.rnn_out, fc_W1), fc_b1))\n",
    "            fc_1_drop = tf.nn.dropout(fc_1, self.keep_prob)\n",
    "            \n",
    "            fc_W2 = tf.get_variable(\n",
    "                    shape=[128, num_classes],\n",
    "                    initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "                    name=\"fc_w2\")\n",
    "            fc_b2 = tf.get_variable(shape=[num_classes], initializer=tf.constant_initializer(0.1), name=\"fc_b2\")\n",
    "            self.logits = tf.squeeze(tf.nn.bias_add(tf.matmul(fc_1_drop, fc_W2), fc_b2), name=\"logits\")\n",
    "        \n",
    "        with tf.variable_scope(\"sigmoid_and_loss\"):\n",
    "            \"\"\"\n",
    "            用sigmoid函数加阈值代替softmax的多分类\n",
    "            \"\"\"\n",
    "            self.sigmoid = tf.nn.sigmoid(self.logits)\n",
    "            self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    logits=self.logits, labels=tf.cast(self.input_y, dtype=tf.float32)))\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch生成器\n",
    "def batch_generator(train_X, train_Y, batch_size, is_train=True):\n",
    "    \"\"\"\n",
    "    batch生成器:\n",
    "    在is_train为true的情况下，补充batch，并shuffle\n",
    "    \"\"\"\n",
    "    data_number = train_X.shape[0]\n",
    "    batch_count = 0\n",
    "    while True:\n",
    "        if batch_count * batch_size + batch_size > data_number:\n",
    "            # 最后一个batch的操作\n",
    "            if is_train:\n",
    "                # 后面的直接舍弃，重新开始\n",
    "                # shuffle\n",
    "                np.random.seed(2018)\n",
    "                trn_idx = np.random.permutation(data_number)\n",
    "                train_X = train_X[trn_idx]\n",
    "                train_Y = train_Y[trn_idx]\n",
    "                one_batch_X = train_X[0:batch_size]\n",
    "                one_batch_Y = train_Y[0:batch_size]\n",
    "                batch_count = 1\n",
    "                yield one_batch_X, one_batch_Y\n",
    "            else:\n",
    "                one_batch_X = train_X[batch_count * batch_size:data_number]\n",
    "                one_batch_Y = train_Y[batch_count * batch_size:data_number]\n",
    "                batch_count = 0\n",
    "                yield one_batch_X, one_batch_Y\n",
    "        else:\n",
    "            one_batch_X = train_X[batch_count * batch_size:batch_count * batch_size + batch_size]\n",
    "            one_batch_Y = train_Y[batch_count * batch_size:batch_count * batch_size + batch_size]\n",
    "            batch_count += 1\n",
    "            yield one_batch_X, one_batch_Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正类欠采样，负类数据增强，暂时用随机打乱数据增强.\n",
    "def data_augmentation(X, Y, under_sample=100000, aug_num=3):\n",
    "    \"\"\"\n",
    "    under_sample: 欠采样个数\n",
    "    aug: 数据增强倍数\n",
    "    \"\"\"\n",
    "    pos_X = []\n",
    "    neg_X = []\n",
    "    for i in range(X.shape[0]):\n",
    "        if Y[i] == 1:\n",
    "            neg_X.append(list(X[i]))\n",
    "        else:\n",
    "            pos_X.append(list(X[i]))\n",
    "    \n",
    "    # 正样本欠采样\n",
    "    random.shuffle(pos_X)\n",
    "    pos_X = pos_X[:-under_sample]\n",
    "    \n",
    "    # 正样本数据增强\n",
    "    pos_X_aug = []\n",
    "    for i in range(200000):\n",
    "        aug = []\n",
    "        for x in pos_X[i]:\n",
    "            if x != 0:\n",
    "                aug.append(x)\n",
    "            else:\n",
    "                break\n",
    "        random.shuffle(aug)\n",
    "        aug += [0] * (max_len-len(aug))\n",
    "        pos_X_aug.append(aug)\n",
    "    pos_X.extend(pos_X_aug)\n",
    "    print(len(pos_X))\n",
    "    \n",
    "    # 负样本数据增强\n",
    "    neg_X_aug = []\n",
    "    for i in range(aug_num):\n",
    "        for neg in neg_X:\n",
    "            aug = []\n",
    "            for x in neg:\n",
    "                if x != 0:\n",
    "                    aug.append(x)\n",
    "                else:\n",
    "                    break\n",
    "            random.shuffle(aug)\n",
    "            aug += [0] * (max_len-len(aug))\n",
    "            neg_X_aug.append(aug)\n",
    "        \n",
    "    neg_X.extend(neg_X_aug)\n",
    "    print(len(neg_X))\n",
    "    \n",
    "    pos_Y = np.zeros(shape=[len(pos_X)], dtype=np.int32)\n",
    "    neg_Y = np.ones(shape=[len(neg_X)], dtype=np.int32)\n",
    "    \n",
    "    pos_X.extend(neg_X)\n",
    "    X_out = np.array(pos_X, dtype=np.int32)\n",
    "    Y_out = np.append(pos_Y, neg_Y)\n",
    "    \n",
    "    print(X_out.shape)\n",
    "    #shuffling the data\n",
    "    np.random.seed(2018)\n",
    "    trn_idx = np.random.permutation(len(X_out))\n",
    "\n",
    "    X_out = X_out[trn_idx]\n",
    "    Y_out = Y_out[trn_idx]\n",
    "    \n",
    "    print(X_out.shape)\n",
    "    print(Y_out.shape)\n",
    "\n",
    "    return X_out, Y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 搜索最佳阈值\n",
    "def bestThreshold(y,y_preds):\n",
    "    tmp = [0,0,0] # idx, cur, max\n",
    "    delta = 0\n",
    "    for tmp[0] in tqdm(np.arange(0.1, 0.501, 0.01)):\n",
    "        tmp[1] = metrics.f1_score(y, np.array(y_preds)>tmp[0])\n",
    "        if tmp[1] > tmp[2]:\n",
    "            delta = tmp[0]\n",
    "            tmp[2] = tmp[1]\n",
    "    print('best threshold is {:.4f} with F1 score: {:.4f}'.format(delta, tmp[2]))\n",
    "    return delta , tmp[2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1306122, 3)\n",
      "Test shape :  (56370, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text  \\\n",
       "0  00002165364db923c7e6  How did Quebec nationalists see their province...   \n",
       "1  000032939017120e6e44  Do you have an adopted dog, how would you enco...   \n",
       "2  0000412ca6e4628ce2cf  Why does velocity affect time? Does velocity a...   \n",
       "3  000042bf85aa498cd78e  How did Otto von Guericke used the Magdeburg h...   \n",
       "4  0000455dfa3e01eae3af  Can I convert montra helicon D to a mountain b...   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00014894849d00ba98a9</td>\n",
       "      <td>My voice range is A2-C5. My chest voice goes u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000156468431f09b3cae</td>\n",
       "      <td>How much does a tutor earn in Bangalore?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000227734433360e1aae</td>\n",
       "      <td>What are the best made pocket knives under $20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0005e06fbe3045bd2a92</td>\n",
       "      <td>Why would they add a hypothetical scenario tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00068a0f7f41f50fc399</td>\n",
       "      <td>What is the dresscode for Techmahindra freshers?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text\n",
       "0  00014894849d00ba98a9  My voice range is A2-C5. My chest voice goes u...\n",
       "1  000156468431f09b3cae           How much does a tutor earn in Bangalore?\n",
       "2  000227734433360e1aae  What are the best made pocket knives under $20...\n",
       "3  0005e06fbe3045bd2a92  Why would they add a hypothetical scenario tha...\n",
       "4  00068a0f7f41f50fc399   What is the dresscode for Techmahindra freshers?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80810\n",
      "(1306122, 50)\n",
      "(1, 50)\n",
      "(56370, 50)\n",
      "185946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(120000, 300)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载数据，平均词向量\n",
    "train_X, test_X, train_Y, local_test_X, local_test_Y, word_index = load_and_prec(use_local_test)\n",
    "\n",
    "# embedding_matrix_1 = load_glove(word_index)\n",
    "embedding_matrix = load_fasttext(word_index)\n",
    "# embedding_matrix = load_para(word_index)\n",
    "# embedding_matrix = np.mean([embedding_matrix_1, embedding_matrix_3], axis = 0)\n",
    "np.shape(embedding_matrix)\n",
    "# embedding_matrix = np.zeros(shape=[100,300],dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold:1\n",
      "WARNING:tensorflow:From <ipython-input-8-176f86a2d4ae>:26: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "seq_dim is deprecated, use seq_axis instead\n",
      "WARNING:tensorflow:From /home/yuhaitao/software/Python3/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py:488: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "batch_dim is deprecated, use batch_axis instead\n",
      "steps:1000, train_loss:0.14694, val_loss:0.11848, time:48.28629\n",
      "steps:2000, train_loss:0.12162, val_loss:0.11336, time:41.64637\n",
      "steps:3000, train_loss:0.11586, val_loss:0.10959, time:42.63949\n",
      "steps:4000, train_loss:0.11292, val_loss:0.10822, time:42.31329\n",
      "steps:5000, train_loss:0.11047, val_loss:0.10684, time:42.92381\n",
      "steps:6000, train_loss:0.10907, val_loss:0.10395, time:42.96681\n",
      "steps:7000, train_loss:0.10659, val_loss:0.10545, time:42.48840\n",
      "steps:8000, train_loss:0.10629, val_loss:0.10385, time:42.57395\n",
      "steps:9000, train_loss:0.10366, val_loss:0.10163, time:42.10168\n",
      "steps:10000, train_loss:0.10411, val_loss:0.10284, time:43.49818\n",
      "test done!\n",
      "steps:11000, train_loss:0.10180, val_loss:0.10079, time:37.79748\n",
      "test done!\n",
      "steps:12000, train_loss:0.10197, val_loss:0.10087, time:20.36755\n",
      "steps:13000, train_loss:0.10065, val_loss:0.10073, time:19.79027\n",
      "test done!\n",
      "steps:14000, train_loss:0.10025, val_loss:0.10098, time:20.49450\n",
      "steps:15000, train_loss:0.09989, val_loss:0.09956, time:19.72200\n",
      "test done!\n",
      "steps:16000, train_loss:0.09849, val_loss:0.09981, time:21.00371\n",
      "steps:17000, train_loss:0.09703, val_loss:0.10119, time:19.74143\n",
      "steps:18000, train_loss:0.09768, val_loss:0.09940, time:19.53834\n",
      "test done!\n",
      "steps:19000, train_loss:0.09666, val_loss:0.10071, time:20.62230\n",
      "steps:20000, train_loss:0.09600, val_loss:0.09879, time:19.60525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 2/41 [00:00<00:01, 19.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 21.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best threshold is 0.3000 with F1 score: 0.6784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 多折训练，交叉验证平均，测试\n",
    "\n",
    "# 划分交叉验证集\n",
    "DATA_SPLIT_SEED = 20190101\n",
    "splits = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=DATA_SPLIT_SEED).split(train_X, train_Y))\n",
    "\n",
    "# test batch\n",
    "test_batch = batch_generator(test_X, np.zeros(shape=[test_X.shape[0]], dtype=np.int32), batch_size, False)\n",
    "local_test_batch = batch_generator(local_test_X, local_test_Y, batch_size, False)\n",
    "\n",
    "# 最终输出\n",
    "train_preds = np.zeros(len(train_X), dtype=np.float32)\n",
    "test_preds = np.zeros((len(test_X), len(splits)), dtype=np.float32)\n",
    "test_preds_local = np.zeros((len(local_test_X), len(splits)), dtype=np.float32)\n",
    "best_threshold = 0.33\n",
    "\n",
    "# 多折训练\n",
    "for i, (train_idx, valid_idx) in enumerate(splits):\n",
    "    print(\"fold:{}\".format(i+1))\n",
    "    X_train = train_X[train_idx]\n",
    "    Y_train = train_Y[train_idx]\n",
    "    X_val = train_X[valid_idx]\n",
    "    Y_val = train_Y[valid_idx]\n",
    "    \n",
    "#     # 数据增强\n",
    "#     X_train, Y_train = data_augmentation(X_train, Y_train) \n",
    "#     print(Y_train[:100])\n",
    "#     print(Y_train[-100:])\n",
    "    \n",
    "    # 训练batch生成器\n",
    "    train_batch = batch_generator(X_train, Y_train, batch_size, True)\n",
    "    val_batch = batch_generator(X_val, Y_val, batch_size, False)\n",
    "    \n",
    "    # 选择最好的结果\n",
    "    best_val_f1 = 0.0\n",
    "    best_val_loss = 99999.99999\n",
    "    best_val_fold = []\n",
    "    best_test_fold = []\n",
    "    best_local_test_fold = []\n",
    "    \n",
    "    # 训练 & 验证 & 测试\n",
    "    with tf.Graph().as_default():\n",
    "        sess_config = tf.ConfigProto(allow_soft_placement=True)\n",
    "        sess_config.gpu_options.allow_growth = True\n",
    "        with tf.Session(config=sess_config) as sess:\n",
    "            writer = tf.summary.FileWriter(\"./log/\", sess.graph)\n",
    "            # 模型\n",
    "            model = model_text_rnn(embedding_matrix=embedding_matrix, sequence_length=max_len)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            train_loss_sum = 0.0\n",
    "            start_time = time.time()\n",
    "            \n",
    "            for go in range(20000):\n",
    "                steps = sess.run(model.global_step) + 1\n",
    "                # 训练\n",
    "                train_batch_X, train_batch_Y = next(train_batch)\n",
    "                feed = {model.input_x:train_batch_X, model.input_y:train_batch_Y, model.keep_prob:0.7}\n",
    "                loss, train_op = sess.run([model.loss, model.train_op], feed_dict=feed)\n",
    "                train_loss_sum += loss\n",
    "                \n",
    "                # 验证 & 测试\n",
    "                if steps % 1000 == 0: \n",
    "                    val_predictions = []\n",
    "                    val_loss_sum = 0.0\n",
    "                    for _ in range(X_val.shape[0] // batch_size + 1):\n",
    "                        val_batch_X, val_batch_Y = next(val_batch)\n",
    "                        feed_val = {model.input_x:val_batch_X, model.input_y:val_batch_Y, model.keep_prob:1.0}\n",
    "                        val_loss, val_sigmoid = sess.run([model.loss, model.sigmoid], feed_dict=feed_val)\n",
    "                        val_predictions.extend(val_sigmoid)\n",
    "                        val_loss_sum += val_loss\n",
    "#                     val_f1 = metrics.f1_score(Y_val, np.array(val_predictions))\n",
    "#                     val_pre = metrics.precision_score(Y_val, np.array(val_predictions))\n",
    "#                     val_recall = metrics.recall_score(Y_val, np.array(val_predictions))\n",
    "                    val_loss_sum = val_loss_sum / (X_val.shape[0] // batch_size + 1)\n",
    "#                     print(\"steps:{}, train_loss:{:.5f}, val_loss:{:.5f}, val_F1:{:.5f}, val_pre:{:.5f}, val_recall:{:.5f}\".format(\n",
    "#                         steps, float(train_loss_sum / 1000), float(val_loss_sum), float(val_f1), float(val_pre), float(val_recall)))\n",
    "                    end_time = time.time()\n",
    "                    print(\"steps:{}, train_loss:{:.5f}, val_loss:{:.5f}, time:{:.5f}\".format(\n",
    "                        steps, float(train_loss_sum / 1000), float(val_loss_sum), end_time-start_time))\n",
    "                    start_time = time.time()\n",
    "                    \n",
    "                    # 写入tensorboard\n",
    "                    train_loss_write = tf.Summary(value=[tf.Summary.Value(tag=\"model/train_loss\", \\\n",
    "                                                                          simple_value=train_loss_sum / 1000), ])\n",
    "                    writer.add_summary(train_loss_write, steps)\n",
    "                    val_loss_write = tf.Summary(value=[tf.Summary.Value(tag=\"model/val_loss\", simple_value=val_loss_sum), ])\n",
    "                    writer.add_summary(val_loss_write, steps)\n",
    "#                     val_f1_write = tf.Summary(value=[tf.Summary.Value(tag=\"index/val_f1\", simple_value=val_f1), ])\n",
    "#                     writer.add_summary(val_f1_write, steps)\n",
    "#                     val_pre_write = tf.Summary(value=[tf.Summary.Value(tag=\"index/val_precision\", simple_value=val_pre), ])\n",
    "#                     writer.add_summary(val_pre_write, steps)\n",
    "#                     val_recall_write = tf.Summary(value=[tf.Summary.Value(tag=\"index/val_recall\", simple_value=val_recall), ])\n",
    "#                     writer.add_summary(val_recall_write, steps)\n",
    "                    writer.flush()\n",
    "                    \n",
    "                    # train loss\n",
    "                    train_loss_sum = 0.0\n",
    "                    \n",
    "#                     # 测试，并选取最好的F1值的时刻的测试结果为最终结果\n",
    "#                     if val_f1 > best_val_f1:\n",
    "#                         best_val_f1 = val_f1\n",
    "#                         best_test = []\n",
    "#                         for _ in range(test_X.shape[0] // batch_size + 1):\n",
    "#                             test_batch_X, _ = next(test_batch)\n",
    "#                             feed_test = {model.input_x:test_batch_X, model.keep_prob:1.0}\n",
    "#                             test_classes = sess.run(model.classes, feed_dict=feed_test)\n",
    "#                             best_test.extend(test_classes)\n",
    "#                         print(\"test done!\")\n",
    "                    \n",
    "                    # 测试，并选取最低的loss值的时刻的测试结果为最终结果\n",
    "                    if val_loss_sum < best_val_loss and steps >= 10000:\n",
    "                        best_val_loss = val_loss_sum\n",
    "                        best_val_fold = val_predictions\n",
    "                        best_test_fold = []\n",
    "                        best_local_test_fold = []\n",
    "                        # 线上test\n",
    "                        for _ in range(test_X.shape[0] // batch_size + 1):\n",
    "                            test_batch_X, _ = next(test_batch)\n",
    "                            feed_test = {model.input_x:test_batch_X, model.keep_prob:1.0}\n",
    "                            test_sigmoid = sess.run(model.sigmoid, feed_dict=feed_test)\n",
    "                            best_test_fold.extend(test_sigmoid)\n",
    "                        # 线下test\n",
    "                        if use_local_test:\n",
    "                            for _ in range(local_test_X.shape[0] // batch_size + 1):\n",
    "                                local_test_batch_X, _ = next(local_test_batch)\n",
    "                                feed_local_test = {model.input_x:local_test_batch_X, model.keep_prob:1.0}\n",
    "                                local_test_sigmoid = sess.run(model.sigmoid, feed_dict=feed_local_test)\n",
    "                                best_local_test_fold.extend(local_test_sigmoid)\n",
    "                        print(\"test done!\")\n",
    "    \n",
    "    \n",
    "    # 更新预测结果\n",
    "    best_threshold, best_f1 = bestThreshold(Y_val, best_val_fold)\n",
    "    \n",
    "#     train_preds[valid_idx] = np.array(best_val_fold)\n",
    "    test_preds[:, i] = np.array(best_test_fold)\n",
    "    if use_local_test:\n",
    "        test_preds_local[:, i] = np.array(best_local_test_fold)\n",
    "#     print(\"fold:{}, threshold:{}, F1_score:{:.5f}\".format(i, best_threshold_fold, \\\n",
    "#             metrics.f1_score(Y_val, (np.array(best_val_fold)>best_threshold_fold).astype(int)))))\n",
    "    # 单模型只测试一折\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 后处理，提交结果\n",
    "\n",
    "if use_local_test:\n",
    "    print(\"local_test_f1:{:.5f}\".format(metrics.f1_score(local_test_Y, (test_preds_local.mean(axis=1) > best_threshold))))\n",
    "\n",
    "sub = pd.read_csv('../input/sample_submission.csv')\n",
    "sub[\"prediction\"] = (test_preds.mean(axis=1)*5 > best_threshold).astype(int)\n",
    "sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1   2   3   4\n",
       "0 NaN NaN NaN NaN NaN\n",
       "1 NaN NaN NaN NaN NaN\n",
       "2 NaN NaN NaN NaN NaN\n",
       "3 NaN NaN NaN NaN NaN\n",
       "4 NaN NaN NaN NaN NaN"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(test_preds_local).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
