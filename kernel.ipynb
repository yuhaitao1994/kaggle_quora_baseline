{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kaggle_quora: 5-fold ensemble of yuhaitao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比赛baseline\n",
    "\n",
    "参考:\n",
    "\n",
    "https://www.kaggle.com/shujian/single-rnn-with-4-folds-clr\n",
    "\n",
    "https://www.kaggle.com/gmhost/gru-capsule\n",
    "\n",
    "https://github.com/dennybritz/cnn-text-classification-tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample_submission.csv', 'test.csv', 'train.csv', 'embeddings', 'embeddings.zip']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "source": [
    "# load package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from collections import Counter\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../input/\"\n",
    "train_file = os.path.join(data_dir, \"train.csv\")\n",
    "test_file = os.path.join(data_dir, \"test.csv\")\n",
    "embedding_size = 300\n",
    "max_len = 50\n",
    "max_features = 120000\n",
    "batch_size = 512\n",
    "use_local_test = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将特殊字符单独挑出\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        if punct in x:\n",
    "#             x = x.replace(punct, f' {punct} ') # 这是python3.6语法\n",
    "            x = x.replace(punct, ' '+punct+' ')\n",
    "    return x\n",
    "\n",
    "# 清洗数字\n",
    "def clean_numbers(x):\n",
    "    if bool(re.search(r'\\d', x)):\n",
    "        x = re.sub('[0-9]{5,}', '#####', x)\n",
    "        x = re.sub('[0-9]{4}', '####', x)\n",
    "        x = re.sub('[0-9]{3}', '###', x)\n",
    "        x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "# 清洗拼写\n",
    "mispell_dict = {\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"I have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\" will\",\n",
    "\"didn't\": \"did not\",\n",
    "\"tryin'\":\"trying\"}\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "    return mispellings_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prec(use_local_test=True):\n",
    "    train_df = pd.read_csv(train_file)\n",
    "    test_df = pd.read_csv(test_file)\n",
    "    print(\"Train shape : \",train_df.shape)\n",
    "    print(\"Test shape : \",test_df.shape)\n",
    "    display(train_df.head())\n",
    "    display(test_df.head())\n",
    "    \n",
    "    # 小写\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].str.lower()\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].str.lower()\n",
    "    \n",
    "    # 数字清洗\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: clean_numbers(x))\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: clean_numbers(x))\n",
    "    \n",
    "    # 清洗拼写\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: replace_typical_misspell(x))\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: replace_typical_misspell(x))\n",
    "    \n",
    "    # 数据清洗\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: clean_text(x))\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: clean_text(x))\n",
    "    \n",
    "    ## fill up the missing values\n",
    "    train_X = train_df[\"question_text\"].fillna(\"_##_\").values\n",
    "    test_X = test_df[\"question_text\"].fillna(\"_##_\").values\n",
    "\n",
    "    ## Tokenize the sentences\n",
    "    # 这个方法把所有字母都小写了\n",
    "    tokenizer = Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list(train_X))\n",
    "    train_X = tokenizer.texts_to_sequences(train_X)\n",
    "    test_X = tokenizer.texts_to_sequences(test_X)\n",
    "    \n",
    "    ## Get the target values\n",
    "    train_Y = train_df['target'].values\n",
    "    print(np.sum(train_Y))\n",
    "    \n",
    "#     # 在pad之前把前30个词去掉\n",
    "#     train_cut = []\n",
    "#     test_cut = []\n",
    "#     for x in train_X:\n",
    "#         train_cut.append([i for i in x if i>30])\n",
    "#     for x in test_X:\n",
    "#         test_cut.append([i for i in x if i>30])\n",
    "#     train_X = train_cut\n",
    "#     test_X = test_cut\n",
    "\n",
    "    ## Pad the sentences \n",
    "    train_X = pad_sequences(train_X, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "    test_X = pad_sequences(test_X, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "    \n",
    "# #     # 把最常用的40个词去掉，pad为0\n",
    "# #     train_X = np.where(train_X>=40, train_X, 0)\n",
    "# #     test_X = np.where(test_X>=40, test_X, 0)\n",
    "    \n",
    "    #shuffling the data\n",
    "    np.random.seed(2019)\n",
    "    trn_idx = np.random.permutation(len(train_X))\n",
    "\n",
    "    train_X = train_X[trn_idx]\n",
    "    train_Y = train_Y[trn_idx]\n",
    "    \n",
    "    # 使用本地测试集\n",
    "    if use_local_test:\n",
    "        train_X, local_test_X = (train_X[:-2*len(test_X)], train_X[-2*len(test_X):])\n",
    "        train_Y, local_test_Y = (train_Y[:-2*len(test_X)], train_Y[-2*len(test_X):])\n",
    "    else:\n",
    "        local_test_X = np.zeros(shape=[1,max_len], dtype=np.int32)\n",
    "        local_test_Y = np.zeros(shape=[1], dtype=np.int32)\n",
    "        \n",
    "    print(train_X.shape)\n",
    "    print(local_test_X.shape)\n",
    "    print(test_X.shape)\n",
    "    print(len(tokenizer.word_index))\n",
    "    \n",
    "    return train_X, test_X, train_Y, local_test_X, local_test_Y, tokenizer.word_index\n",
    "\n",
    "# load_and_prec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(word_index):\n",
    "    EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix\n",
    "\n",
    "def load_fasttext(word_index):\n",
    "    \"\"\"\n",
    "    这个加载词向量还没有细看\n",
    "    \"\"\"\n",
    "    EMBEDDING_FILE = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "    \n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "def load_para(word_index):\n",
    "    EMBEDDING_FILE = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100 and o.split(\" \")[0] in word_index)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "    \n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.eager import context\n",
    "\n",
    "def cyclic_learning_rate(global_step,\n",
    "                         learning_rate=0.001,\n",
    "                         max_lr=0.004,\n",
    "                         step_size=20.,\n",
    "                         gamma=0.99994,\n",
    "                         mode='triangular',\n",
    "                         name=None):\n",
    "  if global_step is None:\n",
    "    raise ValueError(\"global_step is required for cyclic_learning_rate.\")\n",
    "  with ops.name_scope(name, \"CyclicLearningRate\",\n",
    "                      [learning_rate, global_step]) as name:\n",
    "    learning_rate = ops.convert_to_tensor(learning_rate, name=\"learning_rate\")\n",
    "    dtype = learning_rate.dtype\n",
    "    global_step = math_ops.cast(global_step, dtype)\n",
    "    step_size = math_ops.cast(step_size, dtype)\n",
    "    def cyclic_lr():\n",
    "      \"\"\"Helper to recompute learning rate; most helpful in eager-mode.\"\"\"\n",
    "      # computing: cycle = floor( 1 + global_step / ( 2 * step_size ) )\n",
    "      double_step = math_ops.multiply(2., step_size)\n",
    "      global_div_double_step = math_ops.divide(global_step, double_step)\n",
    "      cycle = math_ops.floor(math_ops.add(1., global_div_double_step))\n",
    "      # computing: x = abs( global_step / step_size – 2 * cycle + 1 )\n",
    "      double_cycle = math_ops.multiply(2., cycle)\n",
    "      global_div_step = math_ops.divide(global_step, step_size)\n",
    "      tmp = math_ops.subtract(global_div_step, double_cycle)\n",
    "      x = math_ops.abs(math_ops.add(1., tmp))\n",
    "      # computing: clr = learning_rate + ( max_lr – learning_rate ) * max( 0, 1 - x )\n",
    "      a1 = math_ops.maximum(0., math_ops.subtract(1., x))\n",
    "      a2 = math_ops.subtract(max_lr, learning_rate)\n",
    "      clr = math_ops.multiply(a1, a2)\n",
    "      if mode == 'triangular2':\n",
    "        clr = math_ops.divide(clr, math_ops.cast(math_ops.pow(2, math_ops.cast(\n",
    "            cycle-1, tf.int32)), tf.float32))\n",
    "      if mode == 'exp_range':\n",
    "        clr = math_ops.multiply(math_ops.pow(gamma, global_step), clr)\n",
    "      return math_ops.add(clr, learning_rate, name=name)\n",
    "    if not context.executing_eagerly():\n",
    "      cyclic_lr = cyclic_lr()\n",
    "    return cyclic_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dense layer\n",
    "def dense(inputs, hidden, use_bias=True, \n",
    "          w_initializer=tf.contrib.layers.xavier_initializer(), b_initializer=tf.constant_initializer(0.1), scope=\"dense\"):\n",
    "    \"\"\"\n",
    "    全连接层\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope):\n",
    "        shape = tf.shape(inputs)\n",
    "        dim = inputs.get_shape().as_list()[-1]\n",
    "        out_shape = [shape[idx] for idx in range(\n",
    "            len(inputs.get_shape().as_list()) - 1)] + [hidden]\n",
    "        # 如果是三维的inputs，reshape成二维\n",
    "        flat_inputs = tf.reshape(inputs, [-1, dim])\n",
    "        W = tf.get_variable(\"W\", [dim, hidden], initializer=w_initializer)\n",
    "        res = tf.matmul(flat_inputs, W)\n",
    "        if use_bias:\n",
    "            b = tf.get_variable(\"b\", [hidden], initializer=b_initializer)\n",
    "            res = tf.nn.bias_add(res, b)\n",
    "        # outshape就是input的最后一维变成hidden\n",
    "        res = tf.reshape(res, out_shape)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dot-product attention\n",
    "def dot_attention(inputs, memory, mask, hidden, keep_prob, scope=\"dot_attention\"):\n",
    "    \"\"\"\n",
    "    门控attention层\n",
    "    \"\"\"\n",
    "    def softmax_mask(val, mask):\n",
    "        return -1e30 * (1 - tf.cast(mask, tf.float32)) + val\n",
    "    \n",
    "    with tf.variable_scope(scope):\n",
    "        JX = tf.shape(inputs)[1]  # inputs的1维度，应该是c_maxlen\n",
    "        with tf.variable_scope(\"attention\"):\n",
    "            # inputs_的shape:[batch_size, c_maxlen, hidden]\n",
    "            inputs_ = tf.nn.relu(\n",
    "                dense(inputs, hidden, use_bias=False, scope=\"inputs\"))\n",
    "            memory_ = tf.nn.relu(\n",
    "                dense(memory, hidden, use_bias=False, scope=\"memory\"))\n",
    "            # 三维矩阵相乘，结果的shape是[batch_size, c_maxlen, q_maxlen]\n",
    "            outputs = tf.matmul(inputs_, tf.transpose(\n",
    "                memory_, [0, 2, 1])) / (hidden ** 0.5)\n",
    "            # 将mask平铺成与outputs相同的形状，这里考虑，改进成input和memory都需要mask\n",
    "            mask = tf.tile(tf.expand_dims(mask, axis=1), [1, JX, 1])\n",
    "            logits = tf.nn.softmax(softmax_mask(outputs, mask))\n",
    "            outputs = tf.matmul(logits, memory)\n",
    "            # res:[batch_size, c_maxlen, 12*hidden]\n",
    "            res = tf.concat([inputs, outputs], axis=2)\n",
    "            return res\n",
    "\n",
    "#         with tf.variable_scope(\"gate\"):\n",
    "#             \"\"\"\n",
    "#             attention * gate\n",
    "#             \"\"\"\n",
    "#             dim = res.get_shape().as_list()[-1]\n",
    "#             d_res = dropout(res, keep_prob=keep_prob, is_train=is_train)\n",
    "#             gate = tf.nn.sigmoid(dense(d_res, dim, use_bias=False))\n",
    "#             return res * gate  # 向量的逐元素相乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个多层的双向rnn类，使用cudnn加速, 包括lstm和gru。\n",
    "class cudnn_rnn:\n",
    "    def __init__(self, num_layers, num_units, input_size, neuron=\"GRU\", scope=None):\n",
    "        self.num_layers = num_layers\n",
    "        self.rnns = []\n",
    "        self.scope = scope\n",
    "        self.neuron = neuron\n",
    "        for layer in range(num_layers):\n",
    "            input_size_ = input_size if layer == 0 else 2 * num_units\n",
    "            if self.neuron == \"GRU\":\n",
    "                rnn_fw = tf.contrib.cudnn_rnn.CudnnGRU(1, num_units, name=\"f_cudnn_gru\")\n",
    "                rnn_bw = tf.contrib.cudnn_rnn.CudnnGRU(1, num_units, name=\"b_cudnn_gru\")\n",
    "            elif self.neuron == \"LSTM\":\n",
    "                rnn_fw = tf.contrib.cudnn_rnn.CudnnLSTM(1, num_units, name=\"f_cudnn_lstm\")\n",
    "                rnn_bw = tf.contrib.cudnn_rnn.CudnnLSTM(1, num_units, name=\"b_cudnn_lstm\")\n",
    "            else:\n",
    "                raise NameError\n",
    "            self.rnns.append((rnn_fw, rnn_bw, ))\n",
    "\n",
    "    def __call__(self, inputs, seq_len, keep_prob, concat_layers=True):\n",
    "        # cudnn GRU需要交换张量的维度，可能是便于计算\n",
    "        outputs = [tf.transpose(inputs, [1, 0, 2])]\n",
    "        out_states = []\n",
    "        with tf.variable_scope(self.scope):\n",
    "            for layer in range(self.num_layers):\n",
    "                rnn_fw, rnn_bw = self.rnns[layer]\n",
    "                with tf.variable_scope(\"fw_{}\".format(layer)):\n",
    "                    if self.neuron == \"GRU\":\n",
    "                        out_fw, (fw_state,) = rnn_fw(outputs[-1])\n",
    "                    else:\n",
    "                        out_fw, (fw_state,_) = rnn_fw(outputs[-1])\n",
    "                with tf.variable_scope(\"bw_{}\".format(layer)):\n",
    "                    inputs_bw = tf.reverse_sequence(outputs[-1], seq_lengths=seq_len, seq_dim=0, batch_dim=1)\n",
    "                    if self.neuron == \"GRU\":\n",
    "                        out_bw, (bw_state,) = rnn_bw(outputs[-1])\n",
    "                    else:\n",
    "                        out_bw, (bw_state,_) = rnn_bw(outputs[-1])\n",
    "                    out_bw = tf.reverse_sequence(out_bw, seq_lengths=seq_len, seq_dim=0, batch_dim=1)\n",
    "                outputs.append(tf.concat([out_fw, out_bw], axis=2))\n",
    "                out_states.append(tf.concat([fw_state, bw_state], axis=-1))\n",
    "        if concat_layers:\n",
    "            res = tf.concat(outputs[1:], axis=2)\n",
    "            final_state = tf.squeeze(tf.transpose(tf.concat(out_states, axis=0), [1,0,2]), axis=1)\n",
    "        else:\n",
    "            res = outputs[-1]\n",
    "            final_state = tf.squeeze(out_states[-1], axis=0)\n",
    "        res = tf.transpose(res, [1, 0, 2])\n",
    "        return res, final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_text_cnn(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_matrix, sequence_length=50, num_classes=1,\n",
    "                 embedding_size=300, filter_sizes=[2,3,5], num_filters=256, trainable=True):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.int32, [None], name=\"input_y\")\n",
    "        self.keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "        \n",
    "        # Some variables\n",
    "        self.embedding_matrix = tf.get_variable(\"embedding_matrix\", initializer=tf.constant(\n",
    "                embedding_matrix, dtype=tf.float32), trainable=False)\n",
    "        self.global_step = tf.get_variable('global_step', shape=[], dtype=tf.int32,\n",
    "                                           initializer=tf.constant_initializer(0), trainable=False)\n",
    "        \n",
    "        # The structure of the model\n",
    "        self.layers(num_classes, filter_sizes, num_filters)\n",
    "        # optimizer\n",
    "        if trainable:\n",
    "#             self.learning_rate = tf.train.exponential_decay(\n",
    "#                 learning_rate=0.0015, global_step=self.global_step, decay_steps=1000, decay_rate=0.95)\n",
    "            self.learning_rate = cyclic_learning_rate(\n",
    "                global_step=self.global_step,\n",
    "                step_size=2000)\n",
    "            self.opt = tf.train.AdamOptimizer(learning_rate=self.learning_rate, epsilon=1e-8)\n",
    "            self.train_op = self.opt.minimize(self.loss, global_step=self.global_step)\n",
    "    \n",
    "    def layers(self, num_classes, filter_sizes, num_filters):\n",
    "        # Embedding layer\n",
    "        with tf.variable_scope(\"embedding\"):\n",
    "            self.embedding_inputs = tf.nn.embedding_lookup(self.embedding_matrix, self.input_x)\n",
    "            self.embedding_inputs = tf.nn.dropout(self.embedding_inputs, self.keep_prob)\n",
    "        \n",
    "        with tf.variable_scope(\"convolutional_layers\"):\n",
    "            \"\"\"\n",
    "            卷积和池化操作\n",
    "            \"\"\"\n",
    "            self.pooled_outputs = []\n",
    "            for i, filter_size in enumerate(filter_sizes):\n",
    "                with tf.variable_scope(\"conv-maxpool-{}\".format(filter_size)):\n",
    "                    # conv\n",
    "                    filter_W = tf.get_variable(shape=[filter_size, embedding_size, num_filters],\n",
    "                                               initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                               name=\"filter_W\", dtype=tf.float32)\n",
    "                    filter_b = tf.get_variable(shape=[num_filters], name=\"filter_b\", dtype=tf.float32)\n",
    "                    conv = tf.nn.conv1d(\n",
    "                        value=self.embedding_inputs,\n",
    "                        filters=filter_W,\n",
    "                        stride=1,\n",
    "                        padding=\"VALID\",\n",
    "                        name=\"conv\") # shape:[batch_size, seq_len-filter_size+1, num_filters]\n",
    "                    conv = tf.nn.bias_add(conv, filter_b)\n",
    "                    \n",
    "#                     # batch normalization\n",
    "#                     conv = tf.contrib.layers.batch_norm(conv)\n",
    "                    \n",
    "                    # relu\n",
    "                    h = tf.nn.relu(conv, name=\"relu\")\n",
    "#                     # 原始论文的max-pooling层，只保留1个最大值的信息，就相当于reduce_max\n",
    "#                     pooled_out = tf.reduce_max(h, axis=1, name='global_max_pooling') # shape: [batch_size, num_filters]\n",
    "                    # 使用一个的max-ave-pooling\n",
    "                    pooled = tf.nn.max_pool(\n",
    "                        tf.expand_dims(h[:,:45,:], axis=-1),\n",
    "                        ksize=[1,5,1,1],\n",
    "                        strides=[1,5,1,1],\n",
    "                        padding=\"VALID\",\n",
    "                        name=\"max_pool\") # shape: [batch_size, 9, num_filters, 1]\n",
    "                    pooled_out = tf.reduce_mean(tf.squeeze(pooled, axis=-1), axis=1)\n",
    "                    self.pooled_outputs.append(pooled_out)\n",
    "        \n",
    "            # Combine all the pooled features\n",
    "            self.conv_outputs = tf.concat(self.pooled_outputs, axis=1)\n",
    "            # dropout\n",
    "            self.conv_outputs_drop = tf.nn.dropout(self.conv_outputs, self.keep_prob)\n",
    "        \n",
    "        with tf.variable_scope(\"fully_connected\"):\n",
    "            \"\"\"\n",
    "            全连接层\n",
    "            \"\"\"\n",
    "            fc_1 = dense(inputs=self.conv_outputs_drop, hidden=128, use_bias=True, scope=\"FC_1\")\n",
    "            fc_1 = tf.nn.relu(fc_1)\n",
    "            fc_1_drop = tf.nn.dropout(fc_1, self.keep_prob)\n",
    "            fc_2 = dense(inputs=fc_1_drop, hidden=num_classes, use_bias=True, scope=\"FC_2\")\n",
    "            self.logits = tf.squeeze(fc_2, name=\"logits\")\n",
    "        \n",
    "        with tf.variable_scope(\"sigmoid_and_loss\"):\n",
    "            \"\"\"\n",
    "            用sigmoid函数加阈值代替softmax的多分类\n",
    "            \"\"\"\n",
    "            self.sigmoid = tf.nn.sigmoid(self.logits)\n",
    "            self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    logits=self.logits, labels=tf.cast(self.input_y, dtype=tf.float32)))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_text_rnn(object):\n",
    "    \"\"\"\n",
    "    使用简单的双向GRU实现分类。\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_matrix, sequence_length=50, num_classes=1,\n",
    "                 embedding_size=300, trainable=True):\n",
    "        \n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.int32, [None], name=\"input_y\")\n",
    "        self.keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "        \n",
    "        # Some variables\n",
    "        self.embedding_matrix = tf.get_variable(\"embedding_matrix\", initializer=tf.constant(\n",
    "                embedding_matrix, dtype=tf.float32), trainable=False)\n",
    "        self.global_step = tf.get_variable('global_step', shape=[], dtype=tf.int32,\n",
    "                                           initializer=tf.constant_initializer(0), trainable=False)\n",
    "        \n",
    "        with tf.name_scope(\"process\"):\n",
    "            self.seq_len = tf.reduce_sum(tf.cast(tf.cast(self.input_x, dtype=tf.bool), dtype=tf.int32), axis=1, name=\"seq_len\")\n",
    "        \n",
    "        # The structure of the model\n",
    "        self.layers(num_classes)\n",
    "        # optimizer\n",
    "        if trainable:\n",
    "#             self.learning_rate = tf.train.exponential_decay(\n",
    "#                 learning_rate=0.0015, global_step=self.global_step, decay_steps=1000, decay_rate=0.95)\n",
    "            self.learning_rate = cyclic_learning_rate(\n",
    "                global_step=self.global_step,\n",
    "                step_size=2000)\n",
    "            self.opt = tf.train.AdamOptimizer(learning_rate=self.learning_rate, epsilon=1e-8)\n",
    "            self.train_op = self.opt.minimize(self.loss, global_step=self.global_step)\n",
    "        \n",
    "    def layers(self, num_classes):\n",
    "        # Embedding layer\n",
    "        with tf.variable_scope(\"embedding\"):\n",
    "            self.embedding_inputs = tf.nn.embedding_lookup(self.embedding_matrix, self.input_x)\n",
    "            self.embedding_inputs = tf.nn.dropout(self.embedding_inputs, self.keep_prob)\n",
    "        \n",
    "        # Bi-GRU Encoder\n",
    "        with tf.variable_scope(\"Bi-GRU\"):\n",
    "            bi_rnn = cudnn_rnn(num_layers=1, num_units=128, input_size=self.embedding_inputs.get_shape().as_list()[-1], scope=\"encoder\")\n",
    "            _, final_state = bi_rnn(self.embedding_inputs, seq_len=self.seq_len, keep_prob=self.keep_prob) \n",
    "            # shape: [batch_size, 2*hidden]\n",
    "            self.rnn_out = tf.nn.dropout(final_state, keep_prob=self.keep_prob)\n",
    "            \n",
    "        with tf.variable_scope(\"fully_connected\"):\n",
    "            \"\"\"\n",
    "            全连接层\n",
    "            \"\"\"\n",
    "            fc_1 = dense(inputs=self.rnn_out, hidden=128, use_bias=True, scope=\"FC_1\")\n",
    "            fc_1 = tf.nn.relu(fc_1)\n",
    "            fc_1_drop = tf.nn.dropout(fc_1, self.keep_prob)\n",
    "            \n",
    "            fc_2 = dense(inputs=fc_1_drop, hidden=num_classes, use_bias=True, scope=\"FC_2\")\n",
    "            self.logits = tf.squeeze(fc_2, name=\"logits\")\n",
    "        \n",
    "        with tf.variable_scope(\"sigmoid_and_loss\"):\n",
    "            \"\"\"\n",
    "            用sigmoid函数加阈值代替softmax的多分类\n",
    "            \"\"\"\n",
    "            self.sigmoid = tf.nn.sigmoid(self.logits)\n",
    "            self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    logits=self.logits, labels=tf.cast(self.input_y, dtype=tf.float32)))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_rnn + attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_text_rnn_attention(object):\n",
    "    \"\"\"\n",
    "    使用简单的双向GRU,并接一个attention层。\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_matrix, sequence_length=50, num_classes=1,\n",
    "                 embedding_size=300, trainable=True):\n",
    "        \n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.int32, [None], name=\"input_y\")\n",
    "        self.keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "        \n",
    "        # Some variables\n",
    "        self.embedding_matrix = tf.get_variable(\"embedding_matrix\", initializer=tf.constant(\n",
    "                embedding_matrix, dtype=tf.float32), trainable=False)\n",
    "        self.global_step = tf.get_variable('global_step', shape=[], dtype=tf.int32,\n",
    "                                           initializer=tf.constant_initializer(0), trainable=False)\n",
    "        \n",
    "        with tf.name_scope(\"process\"):\n",
    "            self.seq_len = tf.reduce_sum(tf.cast(tf.cast(self.input_x, dtype=tf.bool), dtype=tf.int32), axis=1, name=\"seq_len\")\n",
    "            self.mask = tf.cast(self.input_x, dtype=tf.bool)\n",
    "        \n",
    "        # The structure of the model\n",
    "        self.layers(num_classes)\n",
    "        # optimizer\n",
    "        if trainable:\n",
    "#             self.learning_rate = tf.train.exponential_decay(\n",
    "#                 learning_rate=0.0015, global_step=self.global_step, decay_steps=1000, decay_rate=0.95)\n",
    "            self.learning_rate = cyclic_learning_rate(\n",
    "                global_step=self.global_step,\n",
    "                step_size=2000)\n",
    "            self.opt = tf.train.AdamOptimizer(learning_rate=self.learning_rate, epsilon=1e-8)\n",
    "            self.train_op = self.opt.minimize(self.loss, global_step=self.global_step)\n",
    "        \n",
    "    def layers(self, num_classes):\n",
    "        # Embedding layer\n",
    "        with tf.variable_scope(\"embedding\"):\n",
    "            self.embedding_inputs = tf.nn.embedding_lookup(self.embedding_matrix, self.input_x)\n",
    "            self.embedding_inputs = tf.nn.dropout(self.embedding_inputs, self.keep_prob)\n",
    "        \n",
    "        # Bi-GRU Encoder\n",
    "        with tf.variable_scope(\"Bi-GRU\"):\n",
    "            bi_rnn = cudnn_rnn(num_layers=1, num_units=128, input_size=self.embedding_inputs.get_shape().as_list()[-1], scope=\"encoder\")\n",
    "            rnn_output, _ = bi_rnn(self.embedding_inputs, seq_len=self.seq_len, keep_prob=self.keep_prob) \n",
    "            # shape: [batch_size, 2*hidden]\n",
    "            self.rnn_out = tf.nn.dropout(rnn_output, keep_prob=self.keep_prob)\n",
    "        \n",
    "        with tf.variable_scope(\"Attention_Layer\"):\n",
    "            \"\"\"\n",
    "            将rnn的输出再做self-attention\n",
    "            \"\"\"\n",
    "            att = dot_attention(inputs=self.rnn_out, memory=self.rnn_out, mask=self.mask, hidden=128,\n",
    "                                keep_prob=self.keep_prob)\n",
    "            # pooling\n",
    "            att_out_1 = tf.reduce_mean(att, axis=2) # shape: [batch_size, 50]\n",
    "            att_out_2 = tf.reduce_max(att, axis=2)\n",
    "            self.att_out = tf.concat([att_out_1, att_out_2], axis=1)\n",
    "            self.att_out = tf.nn.dropout(self.att_out, self.keep_prob)\n",
    "            \n",
    "        with tf.variable_scope(\"fully_connected\"):\n",
    "            \"\"\"\n",
    "            全连接层\n",
    "            \"\"\"\n",
    "            fc = dense(inputs=self.att_out, hidden=num_classes, use_bias=True, scope=\"FC\")\n",
    "            self.logits = tf.squeeze(fc, name=\"logits\")\n",
    "        \n",
    "        with tf.variable_scope(\"sigmoid_and_loss\"):\n",
    "            \"\"\"\n",
    "            用sigmoid函数加阈值代替softmax的多分类\n",
    "            \"\"\"\n",
    "            self.sigmoid = tf.nn.sigmoid(self.logits)\n",
    "            self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    logits=self.logits, labels=tf.cast(self.input_y, dtype=tf.float32)))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text_rcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_text_rcnn(object):\n",
    "    \"\"\"\n",
    "    使用简单的双向GRU,将词的表示扩展为（上文+embedding+下文）的形式，后面再接1-卷积和pooling，也优不接卷积直接pooling的实现。\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_matrix, sequence_length=50, num_classes=1,\n",
    "                 embedding_size=300, trainable=True):\n",
    "        \n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.int32, [None], name=\"input_y\")\n",
    "        self.keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "        \n",
    "        # Some variables\n",
    "        self.embedding_matrix = tf.get_variable(\"embedding_matrix\", initializer=tf.constant(\n",
    "                embedding_matrix, dtype=tf.float32), trainable=False)\n",
    "        self.global_step = tf.get_variable('global_step', shape=[], dtype=tf.int32,\n",
    "                                           initializer=tf.constant_initializer(0), trainable=False)\n",
    "        \n",
    "        with tf.name_scope(\"process\"):\n",
    "            self.seq_len = tf.reduce_sum(tf.cast(tf.cast(self.input_x, dtype=tf.bool), dtype=tf.int32), axis=1, name=\"seq_len\")\n",
    "            self.mask = tf.cast(self.input_x, dtype=tf.bool)\n",
    "        \n",
    "        # The structure of the model\n",
    "        self.layers(num_classes)\n",
    "        # optimizer\n",
    "        if trainable:\n",
    "#             self.learning_rate = tf.train.exponential_decay(\n",
    "#                 learning_rate=0.0015, global_step=self.global_step, decay_steps=1000, decay_rate=0.95)\n",
    "            self.learning_rate = cyclic_learning_rate(\n",
    "                global_step=self.global_step,\n",
    "                step_size=2000)\n",
    "            self.opt = tf.train.AdamOptimizer(learning_rate=self.learning_rate, epsilon=1e-8)\n",
    "            self.train_op = self.opt.minimize(self.loss, global_step=self.global_step)\n",
    "        \n",
    "    def layers(self, num_classes):\n",
    "        # Embedding layer\n",
    "        with tf.variable_scope(\"embedding\"):\n",
    "            self.embedding_inputs = tf.nn.embedding_lookup(self.embedding_matrix, self.input_x)\n",
    "            self.embedding_inputs = tf.nn.dropout(self.embedding_inputs, self.keep_prob)\n",
    "        \n",
    "        # Bi-GRU Encoder\n",
    "        with tf.variable_scope(\"Bi-GRU\"):\n",
    "            bi_rnn = cudnn_rnn(num_layers=1, num_units=128, input_size=self.embedding_inputs.get_shape().as_list()[-1], scope=\"encoder\")\n",
    "            rnn_output, _ = bi_rnn(self.embedding_inputs, seq_len=self.seq_len, keep_prob=self.keep_prob) \n",
    "            self.rnn_out = tf.nn.dropout(rnn_output, keep_prob=self.keep_prob)\n",
    "        \n",
    "        with tf.variable_scope(\"word_representation\"):\n",
    "            \"\"\"\n",
    "            词的上下文表示\n",
    "            \"\"\"\n",
    "            cut = int(self.rnn_out.get_shape().as_list()[-1] / 2)\n",
    "            left = self.rnn_out[:,:,:cut]\n",
    "            right = self.rnn_out[:,:,cut:]\n",
    "            self.word_out = tf.concat([left, self.embedding_inputs, right], axis=2) # shape: [batch_size, 50, 2*128+300]\n",
    "        \n",
    "        with tf.variable_scope(\"text_representation\"):\n",
    "            \"\"\"\n",
    "            text的表示，可以直接线性层加max-pooling，也可以加卷积\n",
    "            \"\"\"\n",
    "            self.text_out = tf.tanh(dense(self.word_out, 128))\n",
    "            self.pool_out = tf.reduce_max(self.text_out, axis=1)\n",
    "            self.pool_out = tf.nn.dropout(self.pool_out, self.keep_prob)\n",
    "            \n",
    "        with tf.variable_scope(\"fully_connected\"):\n",
    "            \"\"\"\n",
    "            全连接层\n",
    "            \"\"\"\n",
    "            fc = dense(inputs=self.pool_out, hidden=num_classes, use_bias=True, scope=\"FC\")\n",
    "            self.logits = tf.squeeze(fc, name=\"logits\")\n",
    "        \n",
    "        with tf.variable_scope(\"sigmoid_and_loss\"):\n",
    "            \"\"\"\n",
    "            用sigmoid函数加阈值代替softmax的多分类\n",
    "            \"\"\"\n",
    "            self.sigmoid = tf.nn.sigmoid(self.logits)\n",
    "            self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    logits=self.logits, labels=tf.cast(self.input_y, dtype=tf.float32)))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM & GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_lstm_gru(object):\n",
    "    \"\"\"\n",
    "    使用简单的双向GRU实现分类。\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_matrix, sequence_length=50, num_classes=1,\n",
    "                 embedding_size=300, trainable=True):\n",
    "        \n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.int32, [None], name=\"input_y\")\n",
    "        self.keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "        \n",
    "        # Some variables\n",
    "        self.embedding_matrix = tf.get_variable(\"embedding_matrix\", initializer=tf.constant(\n",
    "                embedding_matrix, dtype=tf.float32), trainable=False)\n",
    "        self.global_step = tf.get_variable('global_step', shape=[], dtype=tf.int32,\n",
    "                                           initializer=tf.constant_initializer(0), trainable=False)\n",
    "        \n",
    "        with tf.name_scope(\"process\"):\n",
    "            self.seq_len = tf.reduce_sum(tf.cast(tf.cast(self.input_x, dtype=tf.bool), dtype=tf.int32), axis=1, name=\"seq_len\")\n",
    "            self.mask = tf.cast(self.input_x, dtype=tf.bool)\n",
    "        \n",
    "        # The structure of the model\n",
    "        self.layers(num_classes)\n",
    "        # optimizer\n",
    "        if trainable:\n",
    "#             self.learning_rate = tf.train.exponential_decay(\n",
    "#                 learning_rate=0.0015, global_step=self.global_step, decay_steps=1000, decay_rate=0.95)\n",
    "            self.learning_rate = cyclic_learning_rate(\n",
    "                global_step=self.global_step,\n",
    "                step_size=2000)\n",
    "            self.opt = tf.train.AdamOptimizer(learning_rate=self.learning_rate, epsilon=1e-8)\n",
    "            self.train_op = self.opt.minimize(self.loss, global_step=self.global_step)\n",
    "        \n",
    "    def layers(self, num_classes):\n",
    "        # Embedding layer\n",
    "        with tf.variable_scope(\"embedding\"):\n",
    "            self.embedding_inputs = tf.nn.embedding_lookup(self.embedding_matrix, self.input_x)\n",
    "            self.embedding_inputs = tf.nn.dropout(self.embedding_inputs, self.keep_prob)\n",
    "        \n",
    "        # Bi-RNN Encoder\n",
    "        with tf.variable_scope(\"Bi-RNN\"):\n",
    "            # LSTM\n",
    "            bi_lstm = cudnn_rnn(\n",
    "                num_layers=1, num_units=64, input_size=self.embedding_inputs.get_shape().as_list()[-1], neuron=\"LSTM\", scope=\"LSTM\")\n",
    "            self.lstm_out, _ = bi_lstm(self.embedding_inputs, seq_len=self.seq_len, keep_prob=self.keep_prob)\n",
    "            self.lstm_out = tf.nn.dropout(self.lstm_out, keep_prob=self.keep_prob)\n",
    "            # GRU\n",
    "            bi_gru = cudnn_rnn(num_layers=1, num_units=64, input_size=self.lstm_out.get_shape().as_list()[-1], scope=\"GRU\")\n",
    "            self.gru_out, _ = bi_gru(self.lstm_out, seq_len=self.seq_len, keep_prob=self.keep_prob) \n",
    "            self.gru_out = tf.nn.dropout(self.gru_out, keep_prob=self.keep_prob)\n",
    "        \n",
    "        with tf.variable_scope(\"double_attention\"):\n",
    "            lstm_att = dot_attention(\n",
    "                inputs=self.lstm_out, memory=self.lstm_out, mask=self.mask, hidden=128, keep_prob=self.keep_prob, scope=\"l_att\")\n",
    "            gru_att = dot_attention(\n",
    "                inputs=self.gru_out, memory=self.gru_out, mask=self.mask, hidden=128, keep_prob=self.keep_prob, scope=\"g_att\")\n",
    "            # pooling\n",
    "            att_out_lstm = tf.reduce_mean(lstm_att, axis=1) # shape: [batch_size, 256]\n",
    "            att_out_gru = tf.reduce_max(gru_att, axis=1)\n",
    "            self.att_out = tf.concat([att_out_lstm, att_out_gru], axis=1)\n",
    "            \n",
    "        with tf.variable_scope(\"fully_connected\"):\n",
    "            \"\"\"\n",
    "            全连接层\n",
    "            \"\"\"\n",
    "            fc_1 = dense(inputs=self.att_out, hidden=64, use_bias=True, scope=\"FC_1\")\n",
    "            fc_1 = tf.nn.relu(fc_1)\n",
    "            fc_1_drop = tf.nn.dropout(fc_1, self.keep_prob)\n",
    "            \n",
    "            fc_2 = dense(inputs=fc_1_drop, hidden=num_classes, use_bias=True, scope=\"FC_2\")\n",
    "            self.logits = tf.squeeze(fc_2, name=\"logits\")\n",
    "        \n",
    "        with tf.variable_scope(\"sigmoid_and_loss\"):\n",
    "            \"\"\"\n",
    "            用sigmoid函数加阈值代替softmax的多分类\n",
    "            \"\"\"\n",
    "            self.sigmoid = tf.nn.sigmoid(self.logits)\n",
    "            self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    logits=self.logits, labels=tf.cast(self.input_y, dtype=tf.float32)))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fast_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_fastText(object):\n",
    "    \"\"\"\n",
    "    将词向量平均，然后直接全连接层分类。\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_matrix, sequence_length=50, num_classes=1,\n",
    "                 embedding_size=300, trainable=True):\n",
    "        \n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.int32, [None], name=\"input_y\")\n",
    "        self.keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "        \n",
    "        # Some variables\n",
    "        self.embedding_matrix = tf.get_variable(\"embedding_matrix\", initializer=tf.constant(\n",
    "                embedding_matrix, dtype=tf.float32), trainable=False)\n",
    "        self.global_step = tf.get_variable('global_step', shape=[], dtype=tf.int32,\n",
    "                                           initializer=tf.constant_initializer(0), trainable=False)\n",
    "        \n",
    "        with tf.name_scope(\"process\"):\n",
    "            self.seq_len = tf.reduce_sum(tf.cast(tf.cast(self.input_x, dtype=tf.bool), dtype=tf.int32), axis=1, name=\"seq_len\")\n",
    "            self.mask = tf.cast(self.input_x, dtype=tf.bool)\n",
    "        \n",
    "        # The structure of the model\n",
    "        self.layers(num_classes)\n",
    "        # optimizer\n",
    "        if trainable:\n",
    "            self.learning_rate = tf.train.exponential_decay(\n",
    "                learning_rate=0.001, global_step=self.global_step, decay_steps=2000, decay_rate=0.95)\n",
    "            self.opt = tf.train.AdamOptimizer(learning_rate=self.learning_rate, epsilon=1e-8)\n",
    "            self.train_op = self.opt.minimize(self.loss, global_step=self.global_step)\n",
    "        \n",
    "    def layers(self, num_classes):\n",
    "        # Embedding layer\n",
    "        with tf.variable_scope(\"embedding\"):\n",
    "            self.embedding_inputs = tf.nn.embedding_lookup(self.embedding_matrix, self.input_x)\n",
    "#             self.embedding_inputs = tf.nn.dropout(self.embedding_inputs, self.keep_prob)\n",
    "        \n",
    "        with tf.variable_scope(\"pooling\"):\n",
    "            \"\"\"\n",
    "            text的表示，直接将词向量average-pooling\n",
    "            \"\"\"\n",
    "            self.pool_out = tf.reduce_mean(self.embedding_inputs, axis=1)\n",
    "            \n",
    "        with tf.variable_scope(\"fully_connected\"):\n",
    "            \"\"\"\n",
    "            全连接层\n",
    "            \"\"\"\n",
    "            fc_1 = dense(inputs=self.pool_out, hidden=512, use_bias=True, scope=\"FC_1\")\n",
    "            fc_1 = tf.nn.relu(fc_1)\n",
    "            fc_1_drop = tf.nn.dropout(fc_1, self.keep_prob)\n",
    "            \n",
    "            fc_2 = dense(inputs=fc_1_drop, hidden=128, use_bias=True, scope=\"FC_2\")\n",
    "            fc_2 = tf.nn.relu(fc_2)\n",
    "            fc_2_drop = tf.nn.dropout(fc_2, self.keep_prob)\n",
    "            \n",
    "            fc_3 = dense(inputs=fc_2_drop, hidden=num_classes, use_bias=True, scope=\"FC_3\")\n",
    "            self.logits = tf.squeeze(fc_3, name=\"logits\")\n",
    "        \n",
    "        with tf.variable_scope(\"sigmoid_and_loss\"):\n",
    "            \"\"\"\n",
    "            用sigmoid函数加阈值代替softmax的多分类\n",
    "            \"\"\"\n",
    "            self.sigmoid = tf.nn.sigmoid(self.logits)\n",
    "            self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    logits=self.logits, labels=tf.cast(self.input_y, dtype=tf.float32)))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch生成器\n",
    "def batch_generator(train_X, train_Y, batch_size, is_train=True, seed=1234):\n",
    "    \"\"\"\n",
    "    batch生成器:\n",
    "    在is_train为true的情况下，补充batch，并shuffle\n",
    "    \"\"\"\n",
    "    data_number = train_X.shape[0]\n",
    "    batch_count = 0\n",
    "    while True:\n",
    "        if batch_count * batch_size + batch_size > data_number:\n",
    "            # 最后一个batch的操作\n",
    "            if is_train:\n",
    "                # 后面的直接舍弃，重新开始\n",
    "                # shuffle\n",
    "                np.random.seed(seed)\n",
    "                trn_idx = np.random.permutation(data_number)\n",
    "                train_X = train_X[trn_idx]\n",
    "                train_Y = train_Y[trn_idx]\n",
    "                one_batch_X = train_X[0:batch_size]\n",
    "                one_batch_Y = train_Y[0:batch_size]\n",
    "                batch_count = 1\n",
    "                yield one_batch_X, one_batch_Y\n",
    "            else:\n",
    "                one_batch_X = train_X[batch_count * batch_size:data_number]\n",
    "                one_batch_Y = train_Y[batch_count * batch_size:data_number]\n",
    "                batch_count = 0\n",
    "                yield one_batch_X, one_batch_Y\n",
    "        else:\n",
    "            one_batch_X = train_X[batch_count * batch_size:batch_count * batch_size + batch_size]\n",
    "            one_batch_Y = train_Y[batch_count * batch_size:batch_count * batch_size + batch_size]\n",
    "            batch_count += 1\n",
    "            yield one_batch_X, one_batch_Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正类欠采样，负类数据增强，暂时用随机打乱数据增强.\n",
    "def data_augmentation(X, Y, under_sample=100000, aug_num=3):\n",
    "    \"\"\"\n",
    "    under_sample: 欠采样个数\n",
    "    aug: 数据增强倍数\n",
    "    \"\"\"\n",
    "    pos_X = []\n",
    "    neg_X = []\n",
    "    for i in range(X.shape[0]):\n",
    "        if Y[i] == 1:\n",
    "            neg_X.append(list(X[i]))\n",
    "        else:\n",
    "            pos_X.append(list(X[i]))\n",
    "    \n",
    "    # 正样本欠采样\n",
    "    random.shuffle(pos_X)\n",
    "    pos_X = pos_X[:-under_sample]\n",
    "    \n",
    "    # 正样本数据增强\n",
    "    pos_X_aug = []\n",
    "    for i in range(200000):\n",
    "        aug = []\n",
    "        for x in pos_X[i]:\n",
    "            if x != 0:\n",
    "                aug.append(x)\n",
    "            else:\n",
    "                break\n",
    "        random.shuffle(aug)\n",
    "        aug += [0] * (max_len-len(aug))\n",
    "        pos_X_aug.append(aug)\n",
    "    pos_X.extend(pos_X_aug)\n",
    "    print(len(pos_X))\n",
    "    \n",
    "    # 负样本数据增强\n",
    "    neg_X_aug = []\n",
    "    for i in range(aug_num):\n",
    "        for neg in neg_X:\n",
    "            aug = []\n",
    "            for x in neg:\n",
    "                if x != 0:\n",
    "                    aug.append(x)\n",
    "                else:\n",
    "                    break\n",
    "            random.shuffle(aug)\n",
    "            aug += [0] * (max_len-len(aug))\n",
    "            neg_X_aug.append(aug)\n",
    "        \n",
    "    neg_X.extend(neg_X_aug)\n",
    "    print(len(neg_X))\n",
    "    \n",
    "    pos_Y = np.zeros(shape=[len(pos_X)], dtype=np.int32)\n",
    "    neg_Y = np.ones(shape=[len(neg_X)], dtype=np.int32)\n",
    "    \n",
    "    pos_X.extend(neg_X)\n",
    "    X_out = np.array(pos_X, dtype=np.int32)\n",
    "    Y_out = np.append(pos_Y, neg_Y)\n",
    "    \n",
    "    print(X_out.shape)\n",
    "    #shuffling the data\n",
    "    np.random.seed(2018)\n",
    "    trn_idx = np.random.permutation(len(X_out))\n",
    "\n",
    "    X_out = X_out[trn_idx]\n",
    "    Y_out = Y_out[trn_idx]\n",
    "    \n",
    "    print(X_out.shape)\n",
    "    print(Y_out.shape)\n",
    "\n",
    "    return X_out, Y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 搜索最佳阈值\n",
    "def bestThreshold(y,y_preds):\n",
    "    tmp = [0,0,0] # idx, cur, max\n",
    "    delta = 0\n",
    "    for tmp[0] in tqdm(np.arange(0.1, 0.501, 0.01)):\n",
    "        tmp[1] = metrics.f1_score(y, np.array(y_preds)>tmp[0])\n",
    "        if tmp[1] > tmp[2]:\n",
    "            delta = tmp[0]\n",
    "            tmp[2] = tmp[1]\n",
    "    print('best threshold is {:.4f} with F1 score: {:.4f}'.format(delta, tmp[2]))\n",
    "    return delta , tmp[2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed(seed)\n",
    "#     torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1306122, 3)\n",
      "Test shape :  (56370, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text  \\\n",
       "0  00002165364db923c7e6  How did Quebec nationalists see their province...   \n",
       "1  000032939017120e6e44  Do you have an adopted dog, how would you enco...   \n",
       "2  0000412ca6e4628ce2cf  Why does velocity affect time? Does velocity a...   \n",
       "3  000042bf85aa498cd78e  How did Otto von Guericke used the Magdeburg h...   \n",
       "4  0000455dfa3e01eae3af  Can I convert montra helicon D to a mountain b...   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00014894849d00ba98a9</td>\n",
       "      <td>My voice range is A2-C5. My chest voice goes u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000156468431f09b3cae</td>\n",
       "      <td>How much does a tutor earn in Bangalore?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000227734433360e1aae</td>\n",
       "      <td>What are the best made pocket knives under $20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0005e06fbe3045bd2a92</td>\n",
       "      <td>Why would they add a hypothetical scenario tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00068a0f7f41f50fc399</td>\n",
       "      <td>What is the dresscode for Techmahindra freshers?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text\n",
       "0  00014894849d00ba98a9  My voice range is A2-C5. My chest voice goes u...\n",
       "1  000156468431f09b3cae           How much does a tutor earn in Bangalore?\n",
       "2  000227734433360e1aae  What are the best made pocket knives under $20...\n",
       "3  0005e06fbe3045bd2a92  Why would they add a hypothetical scenario tha...\n",
       "4  00068a0f7f41f50fc399   What is the dresscode for Techmahindra freshers?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80810\n",
      "(1193382, 50)\n",
      "(112740, 50)\n",
      "(56370, 50)\n",
      "185946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(120000, 300)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载数据，平均词向量\n",
    "train_X, test_X, train_Y, local_test_X, local_test_Y, word_index = load_and_prec(use_local_test)\n",
    "\n",
    "seed_everything()\n",
    "embedding_matrix_1 = load_glove(word_index)\n",
    "embedding_matrix_2 = load_fasttext(word_index)\n",
    "embedding_matrix_3 = load_para(word_index)\n",
    "embedding_matrix = np.mean([embedding_matrix_1, embedding_matrix_2, embedding_matrix_3], axis = 0)\n",
    "np.shape(embedding_matrix)\n",
    "# embedding_matrix = np.zeros(shape=[100,300],dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold:1\n",
      "steps:10000, train_loss:1.11889, val_loss:0.10480\n",
      "steps:11000, train_loss:0.10298, val_loss:0.10248\n",
      "steps:12000, train_loss:0.09627, val_loss:0.10355\n",
      "steps:13000, train_loss:0.09568, val_loss:0.10280\n",
      "steps:14000, train_loss:0.09538, val_loss:0.10605\n",
      "steps:15000, train_loss:0.09848, val_loss:0.10429\n",
      "steps:16000, train_loss:0.09176, val_loss:0.10271\n",
      "test done!\n",
      "The time of fold 1 is: 307.16953s.\n",
      "fold:2\n",
      "steps:10000, train_loss:1.06643, val_loss:0.10459\n",
      "steps:11000, train_loss:0.09923, val_loss:0.10097\n",
      "steps:12000, train_loss:0.09442, val_loss:0.10001\n",
      "steps:13000, train_loss:0.09258, val_loss:0.10003\n",
      "steps:14000, train_loss:0.09423, val_loss:0.10141\n",
      "steps:15000, train_loss:0.09745, val_loss:0.10505\n",
      "steps:16000, train_loss:0.09251, val_loss:0.09956\n",
      "test done!\n",
      "The time of fold 2 is: 253.50866s.\n",
      "fold:3\n",
      "steps:10000, train_loss:1.07755, val_loss:0.10045\n",
      "steps:11000, train_loss:0.09660, val_loss:0.09859\n",
      "steps:12000, train_loss:0.09074, val_loss:0.09907\n",
      "steps:13000, train_loss:0.09137, val_loss:0.09986\n",
      "steps:14000, train_loss:0.09102, val_loss:0.09975\n",
      "steps:15000, train_loss:0.09458, val_loss:0.09987\n",
      "steps:16000, train_loss:0.08952, val_loss:0.10012\n",
      "test done!\n",
      "The time of fold 3 is: 399.93714s.\n",
      "fold:4\n",
      "steps:10000, train_loss:1.06871, val_loss:0.10498\n",
      "steps:11000, train_loss:0.10045, val_loss:0.10080\n",
      "steps:12000, train_loss:0.09515, val_loss:0.09939\n",
      "steps:13000, train_loss:0.09416, val_loss:0.10050\n",
      "steps:14000, train_loss:0.09502, val_loss:0.10093\n",
      "steps:15000, train_loss:0.09802, val_loss:0.10245\n",
      "steps:16000, train_loss:0.09337, val_loss:0.10133\n",
      "test done!\n",
      "steps:17000, train_loss:0.09319, val_loss:0.10078\n",
      "steps:18000, train_loss:0.09457, val_loss:0.10321\n",
      "The time of fold 4 is: 389.52874s.\n",
      "fold:5\n",
      "steps:10000, train_loss:1.06630, val_loss:0.10003\n",
      "steps:11000, train_loss:0.09786, val_loss:0.10026\n",
      "steps:12000, train_loss:0.09269, val_loss:0.10196\n",
      "steps:13000, train_loss:0.09373, val_loss:0.09929\n",
      "steps:14000, train_loss:0.09381, val_loss:0.10004\n",
      "steps:15000, train_loss:0.09493, val_loss:0.10050\n",
      "steps:16000, train_loss:0.09056, val_loss:0.09910\n",
      "test done!\n",
      "The time of fold 5 is: 474.94775s.\n"
     ]
    }
   ],
   "source": [
    "# 多折训练，交叉验证平均，测试\n",
    "\n",
    "# 随机种子\n",
    "SEED = 6017\n",
    "\n",
    "# 划分交叉验证集\n",
    "splits = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED).split(train_X, train_Y))\n",
    "\n",
    "# test batch\n",
    "test_batch = batch_generator(test_X, np.zeros(shape=[test_X.shape[0]], dtype=np.int32), batch_size, False)\n",
    "local_test_batch = batch_generator(local_test_X, local_test_Y, batch_size, False)\n",
    "\n",
    "# 最终输出\n",
    "train_preds = np.zeros(len(train_X), dtype=np.float32)\n",
    "test_preds = np.zeros((len(test_X), len(splits)), dtype=np.float32)\n",
    "test_preds_local = np.zeros((len(local_test_X), len(splits)), dtype=np.float32)\n",
    "\n",
    "# 多折训练\n",
    "for i, (train_idx, valid_idx) in enumerate(splits):\n",
    "    print(\"fold:{}\".format(i+1))\n",
    "    start_time = time.time()\n",
    "    \n",
    "    X_train = train_X[train_idx]\n",
    "    Y_train = train_Y[train_idx]\n",
    "    X_val = train_X[valid_idx]\n",
    "    Y_val = train_Y[valid_idx]\n",
    "    \n",
    "#     # 数据增强\n",
    "#     X_train, Y_train = data_augmentation(X_train, Y_train) \n",
    "#     print(Y_train[:100])\n",
    "#     print(Y_train[-100:])\n",
    "    \n",
    "    # 训练batch生成器\n",
    "    train_batch = batch_generator(X_train, Y_train, batch_size, True, SEED+i)\n",
    "    val_batch = batch_generator(X_val, Y_val, batch_size, False)\n",
    "    \n",
    "    # 选择最好的结果\n",
    "    best_val_f1 = 0.0\n",
    "    best_val_loss = 99999.99999\n",
    "    best_val_fold = []\n",
    "    best_test_fold = []\n",
    "    best_local_test_fold = []\n",
    "    \n",
    "    # 训练 & 验证 & 测试\n",
    "    with tf.Graph().as_default():\n",
    "        sess_config = tf.ConfigProto(allow_soft_placement=True)\n",
    "        sess_config.gpu_options.allow_growth = True\n",
    "        with tf.Session(config=sess_config) as sess:\n",
    "            \n",
    "            # seed\n",
    "            seed_everything(SEED+i)\n",
    "            tf.set_random_seed(SEED+i)\n",
    "            \n",
    "            # 模型选择， 每一折用不同模型\n",
    "            if i==0:\n",
    "                model = model_text_cnn(embedding_matrix=embedding_matrix, sequence_length=max_len)\n",
    "                KP = 0.7\n",
    "                num_steps = 16000\n",
    "                print_steps = 10000\n",
    "            elif i==1:\n",
    "                model = model_text_rnn(embedding_matrix=embedding_matrix, sequence_length=max_len)\n",
    "                KP = 0.7\n",
    "                num_steps = 16000\n",
    "                print_steps = 10000\n",
    "            elif i==2:\n",
    "                model = model_text_rnn_attention(embedding_matrix=embedding_matrix, sequence_length=max_len)\n",
    "                KP = 0.7\n",
    "                num_steps = 16000\n",
    "                print_steps = 10000\n",
    "            elif i==3:\n",
    "                model = model_text_rcnn(embedding_matrix=embedding_matrix, sequence_length=max_len)\n",
    "                KP = 0.7\n",
    "                num_steps = 18000\n",
    "                print_steps = 10000\n",
    "            else:\n",
    "                model = model_lstm_gru(embedding_matrix=embedding_matrix, sequence_length=max_len)\n",
    "                KP = 0.7\n",
    "                num_steps = 16000\n",
    "                print_steps = 10000\n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            train_loss_sum = 0.0\n",
    "            for go in range(num_steps):\n",
    "                steps = sess.run(model.global_step) + 1\n",
    "                \n",
    "                # 训练\n",
    "                train_batch_X, train_batch_Y = next(train_batch)\n",
    "                feed = {model.input_x:train_batch_X, model.input_y:train_batch_Y, model.keep_prob:KP}\n",
    "                loss, train_op = sess.run([model.loss, model.train_op], feed_dict=feed)\n",
    "                train_loss_sum += loss\n",
    "                \n",
    "                # 验证 & 测试\n",
    "                if steps % 1000 == 0 and steps >= print_steps: \n",
    "                    val_predictions = []\n",
    "                    val_loss_sum = 0.0\n",
    "                    for _ in range(X_val.shape[0] // batch_size + 1):\n",
    "                        val_batch_X, val_batch_Y = next(val_batch)\n",
    "                        feed_val = {model.input_x:val_batch_X, model.input_y:val_batch_Y, model.keep_prob:1.0}\n",
    "                        val_loss, val_sigmoid = sess.run([model.loss, model.sigmoid], feed_dict=feed_val)\n",
    "                        val_predictions.extend(val_sigmoid)\n",
    "                        val_loss_sum += val_loss\n",
    "                    val_loss_sum = val_loss_sum / (X_val.shape[0] // batch_size + 1)\n",
    "                    \n",
    "                    print(\"steps:{}, train_loss:{:.5f}, val_loss:{:.5f}\".format(\n",
    "                        steps, float(train_loss_sum / 1000), float(val_loss_sum)))\n",
    "                    \n",
    "                    # train loss\n",
    "                    train_loss_sum = 0.0\n",
    "                    \n",
    "                    # 测试，并选取最低的loss值的时刻的测试结果为最终结果\n",
    "#                     if val_loss_sum < best_val_loss:\n",
    "                    if steps == 16000:\n",
    "                        best_val_loss = val_loss_sum\n",
    "                        best_val_fold = val_predictions\n",
    "                        best_test_fold = []\n",
    "                        best_local_test_fold = []\n",
    "                        # 线上test\n",
    "                        for _ in range(test_X.shape[0] // batch_size + 1):\n",
    "                            test_batch_X, _ = next(test_batch)\n",
    "                            feed_test = {model.input_x:test_batch_X, model.keep_prob:1.0}\n",
    "                            test_sigmoid = sess.run(model.sigmoid, feed_dict=feed_test)\n",
    "                            best_test_fold.extend(test_sigmoid)\n",
    "                        # 线下test\n",
    "                        if use_local_test:\n",
    "                            for _ in range(local_test_X.shape[0] // batch_size + 1):\n",
    "                                local_test_batch_X, _ = next(local_test_batch)\n",
    "                                feed_local_test = {model.input_x:local_test_batch_X, model.keep_prob:1.0}\n",
    "                                local_test_sigmoid = sess.run(model.sigmoid, feed_dict=feed_local_test)\n",
    "                                best_local_test_fold.extend(local_test_sigmoid)\n",
    "                        print(\"test done!\")\n",
    "                \n",
    "    \n",
    "    # 更新预测结果\n",
    "    train_preds[valid_idx] = np.array(best_val_fold)\n",
    "    test_preds[:, i] = np.array(best_test_fold)\n",
    "    if use_local_test:\n",
    "        test_preds_local[:, i] = np.array(best_local_test_fold)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(\"The time of fold {} is: {:.5f}s.\".format(i+1, end_time-start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:06<00:00,  5.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best threshold is 0.3500 with F1 score: 0.6780\n",
      "local_test_f1:0.69438\n"
     ]
    }
   ],
   "source": [
    "# 后处理，提交结果\n",
    "\n",
    "best_threshold, best_f1 = bestThreshold(train_Y, train_preds)\n",
    "if use_local_test:\n",
    "    print(\"local_test_f1:{:.5f}\".format(metrics.f1_score(local_test_Y, (test_preds_local.mean(axis=1) > best_threshold))))\n",
    "\n",
    "sub = pd.read_csv('../input/sample_submission.csv')\n",
    "sub[\"prediction\"] = (test_preds.mean(axis=1) > best_threshold).astype(int)\n",
    "sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.937434</td>\n",
       "      <td>0.931357</td>\n",
       "      <td>0.934701</td>\n",
       "      <td>0.939181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.937434</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.940900</td>\n",
       "      <td>0.941263</td>\n",
       "      <td>0.944434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.931357</td>\n",
       "      <td>0.940900</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.936034</td>\n",
       "      <td>0.938299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.934701</td>\n",
       "      <td>0.941263</td>\n",
       "      <td>0.936034</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.942658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.939181</td>\n",
       "      <td>0.944434</td>\n",
       "      <td>0.938299</td>\n",
       "      <td>0.942658</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4\n",
       "0  1.000000  0.937434  0.931357  0.934701  0.939181\n",
       "1  0.937434  1.000000  0.940900  0.941263  0.944434\n",
       "2  0.931357  0.940900  1.000000  0.936034  0.938299\n",
       "3  0.934701  0.941263  0.936034  1.000000  0.942658\n",
       "4  0.939181  0.944434  0.938299  0.942658  1.000000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(test_preds_local).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:00<00:00, 70.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best threshold is 0.3600 with F1 score: 0.6945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bt, bf = bestThreshold(local_test_Y, test_preds_local.mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
